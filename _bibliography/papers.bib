---
---
@misc{lee2025collaborativeevaluationdeepfaketext,
      title={Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems}, 
      author={Jooyoung Lee and Xiaochen Zhu and Georgi Karadzhov and Tom Stafford and Andreas Vlachos and Dongwon Lee},
      year={2025},
      eprint={2503.04945},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      selected={true},
      preview={deepfake-dialogue.png},
      abbr={arXiv},
      html={https://arxiv.org/abs/2503.04945},
      abstract="The proliferation of generative models has presented significant challenges in distinguishing authentic human-authored content from deepfake content. Collaborative human efforts, augmented by AI tools, present a promising solution. In this study, we explore the potential of DeepFakeDeLiBot, a deliberation-enhancing chatbot, to support groups in detecting deepfake text. Our findings reveal that group-based problem-solving significantly improves the accuracy of identifying machine-generated paragraphs compared to individual efforts. While engagement with DeepFakeDeLiBot does not yield substantial performance gains overall, it enhances group dynamics by fostering greater participant engagement, consensus building, and the frequency and diversity of reasoning-based utterances. Additionally, participants with higher perceived effectiveness of group collaboration exhibited performance benefits from DeepFakeDeLiBot. These findings underscore the potential of deliberative chatbots in fostering interactive and productive group dynamics while ensuring accuracy in collaborative deepfake text detection."
}

@article{zhu2024segment,
  title={Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models},
  author={Zhu, Xiaochen and Karadzhov, Georgi and Whitehouse, Chenxi and Vlachos, Andreas},
  journal={arXiv preprint arXiv:2412.11333},
  year={2024},
  selected={true},
  preview={SLD-pipeline.png},
  publisher={arXiv},
  abbr={arXiv},
  html={https://arxiv.org/abs/2412.11333},
  abstract="Diffusion models have shown promise in text generation but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion overlooks word-order dependencies and enforces short output windows, while passage-level diffusion struggles with learning robust representation for long-form text. To address these challenges, we propose Segment-Level Diffusion (SLD), a framework that enhances diffusion-based text generation through text segmentation, robust representation training with adversarial and contrastive learning, and improved latent-space guidance. By segmenting long-form outputs into separate latent representations and decoding them with an autoregressive decoder, SLD simplifies diffusion predictions and improves scalability. Experiments on XSum, ROCStories, DialogSum, and DeliData demonstrate that SLD achieves competitive or superior performance in fluency, coherence, and contextual compatibility across automatic and human evaluation metrics comparing with other diffusion and autoregressive baselines. Ablation studies further validate the effectiveness of our segmentation and representation learning strategies."
}

@article{zhu2024conformity,
  title={Conformity in Large Language Models},
  author={Zhu, Xiaochen and Zhang, Caiqi and Stafford, Tom and Collier, Nigel and Vlachos, Andreas},
  journal={arXiv preprint arXiv:2410.12428},
  year={2024},
  selected={true},
  preview={conformity.png},
  publisher={arXiv},
  abbr={arXiv},
  html={https://arxiv.org/abs/2410.12428},
  abstract="The conformity effect describes the tendency of individuals to align their responses with the majority. Studying this bias in large language models (LLMs) is crucial, as LLMs are increasingly used in various information-seeking and decision-making tasks as conversation partners to improve productivity. Thus, conformity to incorrect responses can compromise their effectiveness. In this paper, we adapt psychological experiments to examine the extent of conformity in state-of-the-art LLMs. Our findings reveal that all models tested exhibit varying levels of conformity toward the majority, regardless of their initial choice or correctness, across different knowledge domains. Notably, we are the first to show that LLMs are more likely to conform when they are more uncertain in their own prediction. We further explore factors that influence conformity, such as training paradigms and input characteristics, finding that instruction-tuned models are less susceptible to conformity, while increasing the naturalness of majority tones amplifies conformity. Finally, we propose two interventions--Devil's Advocate and Question Distillation--to mitigate conformity, providing insights into building more robust language models."
}

@inproceedings{zhu-etal-2023-convlab,
    title = "{C}onv{L}ab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format",
    author = "Zhu, Qi  and
      Geishauser, Christian  and
      Lin, Hsien-chin  and
      van Niekerk, Carel  and
      Peng, Baolin  and
      Zhang, Zheng  and
      Feng, Shutong  and
      Heck, Michael  and
      Lubis, Nurul  and
      Wan, Dazhen  and
      Zhu, Xiaochen  and
      Gao, Jianfeng  and
      Gasic, Milica  and
      Huang, Minlie",
    editor = "Feng, Yansong  and
      Lefever, Els",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-demo.9",
    doi = "10.18653/v1/2023.emnlp-demo.9",
    abbr={EMNLP},
    preview={convlab-3.png},
    pages = "106--123",
    selected={true},
    abstract = "Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short in delivering comprehensive arrays of data, model, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and models, significantly reducing complexity and cost for studying generalization and transfer. Enhanced with robust reinforcement learning (RL) tools, featuring a streamlined training process, in-depth evaluation tools, and a selection of user simulators, ConvLab-3 supports the rapid development and evaluation of robust dialogue policies. Through an extensive study, we demonstrate the efficacy of transfer learning and RL and showcase that ConvLab-3 is not only a powerful tool for seasoned researchers but also an accessible platform for newcomers.",
}

@article{Forward_Zhu_2023, title={Speaking with our Sources— The Possibilities and Pitfalls of {AI} Language Models in Historical Research}, volume={54}, number={2}, journal={PASSPORT THE SOCIETY FOR HISTORIANS OF AMERICAN FOREIGN RELATIONS REVIEW}, author={Forward, Jacob and Zhu, Xiaochen}, year={2023}, month={Sep}, pages={106–107}, selected={true}
} 