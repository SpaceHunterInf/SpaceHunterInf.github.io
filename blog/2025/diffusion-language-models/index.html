<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What are Diffusion Language Models? | Xiaochen Zhu </title> <meta name="author" content="Xiaochen Zhu"> <meta name="description" content="A gentle, in-depth introduction of existing diffusion language models."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/sailfish.jpeg?abf6a8a7b6d5c9b17aa67d2b06b55ab0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Xiaochen</span> Zhu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">What are Diffusion Language Models?</h1> <p class="post-meta"> Created in April 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="preface"><strong>Preface</strong></h4> <p><a href="https://www.youtube.com/watch?v=X0Jti9F-oQA" rel="external nofollow noopener" target="_blank">Dear Reader</a>, I feel like writing this blog for a long time. Diffusion model for language generation is an exciting and emerging field that receives increasing attention. However, up until the moment I am writing this blog, there isn’t a comprehensive guide/intro covering such topics for the members of NLP/ML community who wish to conduct research in this area, prospectively. In this blog, we will walk through the history of diffusion language model, different paradigms in implementation, possible future research directions and applications (<em>also a few of my personal opinion, which might be biased, in italics</em>). The blog will be updated constantly, and I will probably turn this into a survey paper when the time is right.</p> <p>This blog targets audience who already has sufficient knowledge in Diffusion Models, and traditional autoregressive LLMs. If you don’t, no worries, here are resources I found extremely helpful.</p> <ul> <li> <strong>For Diffusion Models</strong>: <ul> <li> <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">What are Diffusion Models?</a> by Lilian Weng <em>Strongly recommended</em> </li> <li> <a href="https://arxiv.org/abs/2208.11970" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a> by Calvin Luo</li> </ul> </li> <li> <strong>For Autoregressive LLMs</strong>: <ul> <li> <a href="https://www.youtube.com/watch?v=LWMzyfvuehA&amp;list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&amp;index=8" rel="external nofollow noopener" target="_blank">Stanford CS224N</a> <em>GOAT course</em> </li> </ul> </li> </ul> <h4 id="whats-diffusion-language-model-dlm"><strong>What’s diffusion language model (DLM)?</strong></h4> <p>A brief recap on all the trending autoregressive language model (AR-LM) nowadays, from GPT-2, Llama, to Gemini, ChatGPT, and Claude, these models are using transformer as the common backbone for <strong>auoregressive</strong> decoding (AR), that is predicting token by token in an left-to-right fashion. By contrast, diffusion language model (DLM) is iteratively refines and predicts the entire sequence from a sampled noisy input as <strong>non-autoregressive</strong> decoding (NAR). A straightforward (<em>simplified</em>) demonstration of the difference between two paradigms can be shown in the pipeline figure below.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion_vs_ar-480.webp 480w,/assets/img/diffusionlm_blog/diffusion_vs_ar-800.webp 800w,/assets/img/diffusionlm_blog/diffusion_vs_ar-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/diffusion_vs_ar.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. AR-LM predicted the sequence token-by-token. Output tokens are used as input for the next token-prediction in left-to-right manner (top). DLM iteratively refines the entire output sequence from a noisy sample (bottom). </div> <p>Putting it into mathematical term is, given a sequence we want to predict, \(\mathbf{x} = \{x_1, x_2, \ldots x_N\}\) , AR-LM with parameter \(\theta\) models the following distribution. \(\begin{equation} \label{eq:AR-LM} P(\mathbf{x}; \theta) = \prod_{t=1}^{N} P(x_n | \mathbf{x}_{&lt;n}; \theta) \end{equation}\)</p> <p>Where DLM take a holistic view of the entire sequence, it models the following distribution, where \(t\) is the timestep in <strong>reverse diffusion process</strong> (<em>we will get to the details very soon</em>). A larger \(t\) means noiser sequence, closer to sampled from a standard gaussian noise. Consider we are having a very messy paragraph in the beginning and iteratively refines into the passage we want.</p> \[\begin{equation} \label{eq:DLM} \mathbf{x}_{t-1} \sim p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, t) \end{equation}\] <h4 id="why-we-need-dlm-or-nar-whats-wrong-with-ar"><strong>Why we need DLM (or NAR)? What’s wrong with AR?</strong></h4> <p>Autoregressive model is extremely successful nowadays, so why do we still need another paradigm? What makes DLM a field worth looking into? Well here are some arguments for you to consider.</p> <ul> <li> <strong>Inherent Disadvantage of AR-LM</strong> <ul> <li> <strong>Error Propagation</strong> For autoregressive models, if you made mistake in predicting the current token, there is no chance for going back for revision. Future predictions are always based on this flawed prediction, propagating and accumulating such errors. We call such struggle as <a href="https://aclanthology.org/D18-1396/" rel="external nofollow noopener" target="_blank">error propagation</a>.</li> <li> <strong>Indirect Generation Control</strong> Current strategies for AR controlled generation depends on extensive training or hacks in decoding strategy. Such techniques are often indirect and inconvienient for users to control the generation. For example, if you want to generate a passage with certain length, you need to train a length predictor or use heuristics like <a href="https://arxiv.org/abs/1904.09751" rel="external nofollow noopener" target="_blank">top-k sampling</a> to control the generation. And what’s more, there is no guarantee 😥.</li> </ul> </li> <li> <strong>(Potential) Advantage of DLM</strong> <ul> <li> <strong>NAR</strong> As sequence are generated holistically, you can fix and correct previous mistakes and refine the sequence as a whole.</li> <li> <strong>Controllability</strong> Diffusion models are known to provide simple, training efficient style controlls using either classifier free guidance or classifier based guidance. Such controllability can be easily extended to DLM, where we can control the style of generation using the same techniques (<a href="https://arxiv.org/abs/2105.05233" rel="external nofollow noopener" target="_blank">Prafulla et al., 2021</a>, <a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">Radford et al., 2021</a>). We can also take this a step further to even more fine-grained controles in lengths, specific text editing, infillings because the model works on the whole sequence representation iteratively (<a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al, 2022</a>, <a href="https://arxiv.org/abs/2502.09992" rel="external nofollow noopener" target="_blank">Nie et al., 2025</a>,). This may also extends to tasks with stronger structural constraints (e.g., code, table).</li> <li> <strong>Diversity</strong> Can produce more diverse outputs compared to beam search in AR. <em>You just need to sample different initial noise.</em> </li> <li> <strong>Speed Up</strong> Poential for faster generation as stops <em>could</em> be paralleilized, as tokens are generated all-together, instead of waiting for previous outputs iteratively.</li> </ul> </li> </ul> <h4 id="diffusion-model-recap"><strong>Diffusion Model Recap</strong></h4> <p>Diffusion models are very successful and widely adopted in computer vision tasks, such as image generation, super-resolution, and inpainting. The core idea of diffusion models is to learn a generative model by reversing a diffusion process that gradually adds noise to the data. Using the famous <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">DDPM</a> as an example, given a data sample from a real data distribution \(\mathbf{x}_0 \sim \mathcal{D}(x)\), we use a <strong>forward process</strong> to gradually perturb the data with small amounts of Gaussian noise over \(T\) steps:</p> <p>\(\begin{equation} q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \end{equation}\) \(\begin{equation} q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}) \end{equation}\)</p> <p>where \(\beta_t \in (0, 1)\) is a variance schedule that controls the amount of noise added at each step. As \(T \to \infty\), \(\mathbf{x}_T\) approaches a sample from a standard Gaussian distribution:</p> \[\begin{equation} \lim_{T \to \infty} \mathbf{x}_T \approx \mathcal{N}(0, \mathbf{I}) \end{equation}\] <p>The <strong>reverse process</strong> then learns to gradually denoise the data, starting from pure noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\) and working backwards, where \(\mu_\theta\) and \(\Sigma_\theta\) are learned by a fancy neural network model. Again, if you are not comfortable with these concepts, please refer to <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">Lilian’s amazing blog</a>.</p> \[\begin{equation} p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \end{equation}\] \[\begin{equation} p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)) \end{equation}\] <div class="row mt-2"> <div class="col-sm-8 col-md-6 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/DDPM-480.webp 480w,/assets/img/diffusionlm_blog/DDPM-800.webp 800w,/assets/img/diffusionlm_blog/DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/DDPM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Ho et al. 2020</a> with a few additional annotations) </div> <h4 id="whats-the-fundamental-challenge"><strong>What’s the fundamental challenge?</strong></h4> <p>Well, if the Diffusion Model is well-established and it has all these exciting perks, why this is not as trending as in the field of computer vision? Good spot, now it comes to the fundamental challenge, the discrepancy between traditional <strong>continuous</strong> diffusion models, which achieved great success in image generation (such as with Denoising Diffusion Probabilistic Models - <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Ho et al., 2020</a>), and the domain of <strong>discrete text</strong>.</p> <p>Think about an image. It’s made of pixels, and each pixel has colour values (like RGB) that are essentially numbers on a continuous scale. Adding “noise” is intuitive: you can slightly perturb these numerical values, typically by adding small random numbers drawn from a Gaussian (bell curve) distribution. Gradually adding more noise smoothly transitions the image to random static. The reverse process involves training a model to predict and subtract that small amount of noise at each step, gradually recovering the image.</p> <p>Now, consider text. Language is composed of discrete units – words or tokens selected from a finite vocabulary (e.g., “cat”, “dog”, “runs”). You can’t simply add “0.1 Gaussian noise” to the word “cat” and get something meaningful that’s slightly different but still related in a smooth, continuous way. Applying the original continuous noise formulation directly just doesn’t work.</p> <p>This <strong>discrete nature</strong> of text is the core hurdle that requires specific adaptations. How do you define a “forward process” that gradually corrupts text into noise, and critically, a “reverse process” that a model can learn to invert step-by-step?</p> <p>Researchers have developed clever workarounds to bridge this gap:</p> <ol> <li> <strong>Operating on Continuous Variables:</strong> One approach is to work not with the tokens themselves, but with their continuous variables. Traditional langauge models gives well-constructed word embedding and hidden outputs representations. We could leverage these representations to define a continuous forward process, where the model learns to predict the noise added to these continuous representations at each step. This is similar to how diffusion models operate in the image domain, where the forward process is defined in the latent space of a VAE or similar architecture. <ul> <li> <strong>Word Embedding (Token Level)</strong> Noise <em>can</em> be added to these word embedding vectors, a technique used in models like <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Diffusion-LM</a>, and the pre-trained DLM <a href="https://arxiv.org/abs/2212.11685" rel="external nofollow noopener" target="_blank">GENIE</a>. However, mapping potentially noisy embeddings back to specific discrete tokens at each step introduces its own complexities.</li> <li> <strong>Higher Level Latent Representations</strong> Works like <a href="https://arxiv.org/abs/2306.02531" rel="external nofollow noopener" target="_blank">PLANNER</a> and <a href="https://arxiv.org/pdf/2212.09462" rel="external nofollow noopener" target="_blank">LD4LG</a> are operating on a higher level latent representations of paragraphs of texts. However, the latent representations are fragile when perturbed by noise, resulting in abrupt semantic changes in reverse diffusion process. <strong>My own paper</strong> <a href="https://arxiv.org/abs/2412.11333" rel="external nofollow noopener" target="_blank">SLD</a> attempts to mitigate this issue <em>cleverly</em> by text-segementation and improved representation learning techniques. Meta’s <a href="https://arxiv.org/pdf/2412.08821" rel="external nofollow noopener" target="_blank">Large Concept Model</a> is probably the <em>ultimate form</em> of pre-trained DLM following this path.</li> </ul> </li> <li> <strong>Discrete Corruption Surrogate:</strong> More commonly, diffusion models for text define analogous discrete “noising” or “corruption” processes, as explored in papers like . Instead of adding mathematical noise, the forward process might involve: <ul> <li> <strong>Masking:</strong> Randomly replacing tokens with a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token, with the number of masked tokens increasing over diffusion steps. The recent trending work of <a href="https://arxiv.org/pdf/2502.09992" rel="external nofollow noopener" target="_blank">LLaDA</a>, is the pre-trained DLM (8B parameters, largest of all kinds) following this path.</li> <li> <strong>Token Substitution:</strong> Randomly replacing tokens with other tokens from the vocabulary.</li> <li> <strong>Hybrids:</strong> Combining these or other discrete corruption methods.</li> </ul> </li> <li> <strong>Discrete Diffusion:</strong> Now a few <em>bold geniuses</em> are thinking, “Hey, if tokens are discrete, why not make the diffusion process discrete as well?”. So we have discrete diffusion over categorical supports. <a href="https://arxiv.org/pdf/2102.05379" rel="external nofollow noopener" target="_blank">Eminel et al, 2021</a> introduces extensions of diffusion for categorical data such as language. We model each token as a probability mass vector distributing over \(p \in \mathbb{R}^{V}\), where \(V\) is the size of the vocabulary and use transition matrices \(\mathbf{Q}\) to model transitions between denoising timestops, for example \(q(\mathbf{x}_t |\mathbf{x}_{t-1}) = Categorical(\mathbf{x}_t; \mathbf{p}=\mathbf{x}_{t-1}\mathbf{Q}_t)\). Models like <a href="https://arxiv.org/abs/2107.03006" rel="external nofollow noopener" target="_blank">D3PM</a>, and <a href="https://arxiv.org/pdf/2310.16834" rel="external nofollow noopener" target="_blank">SEDD</a> (<em>ICML 2024 Best Paper</em>) follows this path.</li> <li>🔥🖼️👊 <strong>The Maverick–Text in Image:</strong> <em>“Discrete text? What text? It’s an image!”</em> I personally wish you to checkout this <a href="https://arxiv.org/pdf/2304.12519" rel="external nofollow noopener" target="_blank">GlyphDiffusion</a>. Instead of dealing with the discrepancy, they bypassed by rendering the target text as a glyph image containing visual language content 🤣.</li> </ol> <p>The reverse process then becomes about learning to <strong>undo</strong> this specific type of discrete corruption. For instance, the model learns to predict the original tokens at the <code class="language-plaintext highlighter-rouge">[MASK]</code> positions or identify and correct a randomly sampled variable into meaningful tokens , iteratively refining the sequence from a highly corrupted state back to coherent text. So, while the core <em>idea</em> of diffusion (iterative refinement from noise) remains, the <em>mechanisms</em> for the forward (corruption) and reverse (denoising) processes have to be specifically adapted for the discrete world of language.</p> <p>Now I will select the representative work of each of these methods to further explain the concept of DLM.</p> <h4 id="embedding-level-diffusion--where-it-begins"><strong>Embedding-Level diffusion – where it begins</strong></h4> <p><em>As far as I know</em>, <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Diffusion-LM</a> is probably the first, influencial work that starts the era of DLM. Now suppose we have a sequence of words \(\mathbf{w} = \{w_1, w_2, \ldots, w_n\}\), an embedding fucntion is defined to map each word to a word vector \(Emb(w_i) \in \mathbb{R}^{d}\). So the entire sequence weill be encoded into \(\mathbf{x}_0 = Emb(\mathbf{w}) \in \mathbb{R}^{d}\). So yeah! There we go, we have a <strong>continuous</strong> space where we can run the conventional diffusion models. We use the typical simplified KL-divergence term in evidence lower bound (which I will not reiterate here) to derive a loss function.</p> \[\begin{equation} \mathcal{L}_{simple}(\mathbf{x}_0) = \Sigma_{t=1}^T \underset{q(\mathbf{x}_t | \mathbf{x}_0)}{\mathbb{E}} ||\mu(\mathbf{x}_t, t) - \hat{\mu}(\mathbf{x}_t, \mathbf{x}_0)||^2 \end{equation}\] <p>But don’t forget, we need to convert the embedding back to discrete tokens, you will say this is easy, let’s just have another function transform them back to tokens. Indeed, that’s how it’s done. In Li’s implementation, they model theses steps into the diffusion process as an extra timestep. As shown in the figure below, the forward process consists of an additional Markov transition to obtain the embeddings parametrized by \(q_{\phi}(\mathbf{x}_0 | \mathbf{w}) = \mathcal{N}(Emb(\mathbf{w}); \sigma_{0} I)\) . Then in the reverse process you will have an additional trainble <strong>rounding</strong> step, parameterized by \(p_{\theta}(\mathbf{w} | \mathbf{x}_0) = \prod_{i=1}^n p_{\theta}(w_i | x_i)\) , where \(p_{\theta}(w_i | x_i)\) is a simple softmax distribution.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/diffusion-lm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. A graphical model representing the forward and reverse diffusion processes for Diffusion-LM. (Image source: <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al. 2022</a>) </div> <p>Then we can adjust the loss function accordingly. For end-to-end training, we will have the final loss function as below. During the reverse process, you run the inferece by sampling a random embedding sequence containing \(n\) token embeddings (fixed, the same as in the training process) and gradually remove the noise.</p> \[\begin{equation} \mathcal{L}_{simple}^{e2e}(\mathbf{w}) = \underset{q_{\phi}(\mathbf{x}_{0:T} | \mathbf{w})}{\mathbb{E}} \left[\underbrace{\mathcal{L}_{simple}(\mathbf{x}_0)}_{\text{diffusion Loss}} + ||Emb(\mathbf{w}) - \overbrace{\mu_{\theta}(\mathbf{x}_1, 1)}^{\text{predicted }\mathbf{x}_0}||^2 - \underbrace{\log p_{\theta}(\mathbf{w} | \mathbf{x}_0)}_{\text{rounding}} \right] \end{equation}\] <p>So, everything seems extremely straightforward right? Or does it? Unfortunately, no 😅. The conversion between continous embedding space and discrete tokens is non-trivial, harder than you think. <span style="color:red">This rounding is a key challenge in token embedding-level diffusion models. The discretization step can lead to errors that accumulate across the diffusion process, as the embedding space is not uniformly populated with valid tokens.</span> <em>Well isn’t this the notorious data sparsity revisited.</em></p> <p>In the paper, there is a major chapter talking about the techniques of how they managed to reduce the rounding error to obtain admissible outputs. For example using reparameterisation trick to make sure every term in the loss models \(\mathbf{x}_0\) explicitly. They also introduce a <strong>clamping trick</strong> that is maping the predicted vector \(\mathbf{x}_t\) to its nearest word embedding sequence in every reverse diffusion sampling step. Still, a lot of work needs to be done in the future for the sake of generation quality.</p> <p>Back to the model, with the diffusion pipeline, you can do the fancy conditioning and controlled generation during your inference now. For example, you could have a separate neural network classifier and a class condition \(\mathbf{c}\). During the backward process, you obtain the \(\mathbf{x}_{t-1}\) with respect to the posterior probability using the gradient update below.</p> \[\begin{equation} \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{c}) = \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}) + \underbrace{\nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{c} | \mathbf{x}_{t-1})}_{\text{classifier guidance}} \end{equation}\] <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-classifier-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/diffusion-lm-classifier.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. For controllable generation, we iteratively perform gradient updates on these continuous latents to optimize for fluency (parametrized by Diffusion-LM) and satisfy control requirements (parametrized by a classifier). (Image source: <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al. 2022</a>) </div> <p>The paper provides many experiments for controlled generation (e.g., semantics, length, POS and etc.). I want to mention about <strong>infilling</strong> specifically. That is during the inference process, instead of denoising all the embeddings, some context embeddings are given and fixed (e.g., \(\mathbf{x}_t =\)<code class="language-plaintext highlighter-rouge">[w_1] [noise] [w_2]</code>), the diffusion model will mask the gradient of other tokens, only generated the noised token in the middle. However, the other embeddings are used together as conditions naturally during the reverse sampling, as a <strong>classifier-free guidance</strong>. And you, my clever reader, immediately understands how traditional sequence-to-sequence task can be modelled by giving the input as left contexts only.</p> <p><a href="https://arxiv.org/pdf/2212.11685" rel="external nofollow noopener" target="_blank">GENIE</a> is the first pre-trained DLM (<em>again, as I know</em>) following this token embedding-level diffusion. The model uses <strong>continuous paragraph denoise</strong> objective for pre-training. The object first applied a hybrid noising techniques to the original text, including token masking (e.g., <code class="language-plaintext highlighter-rouge">[w_1] [MASK] [w_2]</code>) and forward diffusion process sampling, then ask the model to recover the clean original text.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GENIE-480.webp 480w,/assets/img/diffusionlm_blog/GENIE-800.webp 800w,/assets/img/diffusionlm_blog/GENIE-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/GENIE.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5. The framework of GENIE. Source sequence is encoded as the condition of the transformer DLM through cross attention. DLM restores the randomly initial Gaussian noise to the output text through the iterative denoising and grounding process (Image source: <a href="https://arxiv.org/pdf/2212.11685" rel="external nofollow noopener" target="_blank">Lin et al., 2022</a>) </div> <p>Notably, GENIE doesn’t use infilling as the default sequence-to-sequence generation method. Instead, it is using a more Encoder-Decoder approach (yes, like BART or T5), which is another analogues of classifier-free guidance. Similar rounding techniques is applied, they are using an effective KNN algorithm to retreive closets word embedding of each token during reverse sampling and apply rounding to the closest learnt word embedding.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"SpaceHunterInf/SpaceHunterInf.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xiaochen Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"cv",description:"Here is my CV, updated in March 2025.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-what-are-diffusion-language-models",title:"What are Diffusion Language Models?",description:"A gentle, in-depth introduction of existing diffusion language models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion-language-models/"}},{id:"post-a-post-with-image-galleries",title:"a post with image galleries",description:"this is what included image galleries could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/photo-gallery/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/SpaceHunterInf","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/neo-zhu-01270824b","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=1tkzwd4AAAAJ","_blank")}},{id:"social-x",title:"X",section:"Socials",handler:()=>{window.open("https://twitter.com/ZhuNeo13294","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>