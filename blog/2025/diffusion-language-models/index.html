<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What are Diffusion Language Models? | Xiaochen Zhu </title> <meta name="author" content="Xiaochen Zhu"> <meta name="description" content="A gentle, in-depth introduction of existing diffusion language models."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/sailfish.jpeg?abf6a8a7b6d5c9b17aa67d2b06b55ab0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Xiaochen</span> Zhu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">What are Diffusion Language Models?</h1> <p class="post-meta"> Created in April 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="preface"><strong>Preface</strong></h4> <p><a href="https://www.youtube.com/watch?v=X0Jti9F-oQA" rel="external nofollow noopener" target="_blank">Dear Reader</a>,<br> I’ve been wanting to write this blog for a long time. Diffusion models for language generation are super exciting — an emerging field that’s getting increasing attention. But up until now, there hasn’t really been a comprehensive guide or intro for folks in the NLP/ML community who want to dive into this area and maybe even start doing research. So here we are! In this blog, we’ll walk through the history of diffusion language models, different paradigms for building them, some future research directions and applications — <em>plus a few of my own (possibly biased) personal opinions, italicized for your reading pleasure.</em> I’ll also keep updating this blog over time, and hey, who knows — maybe it’ll grow into a full survey paper one day.</p> <p>This blog is mainly for people who already know a decent bit about Diffusion Models and good old autoregressive LLMs. If that’s not you yet, no worries — here are some resources I found super helpful:</p> <ul> <li> <strong>For Diffusion Models</strong>: <ul> <li> <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">What are Diffusion Models?</a> by Lilian Weng (<em>strongly recommended!</em>)</li> <li> <a href="https://arxiv.org/abs/2208.11970" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a> by Calvin Luo</li> </ul> </li> <li> <strong>For Autoregressive LLMs</strong>: <ul> <li> <a href="https://www.youtube.com/watch?v=LWMzyfvuehA&amp;list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&amp;index=8" rel="external nofollow noopener" target="_blank">Stanford CS224N</a> (<em>GOAT course, trust me</em>)</li> </ul> </li> </ul> <hr> <h4 id="whats-a-diffusion-language-model-dlm"><strong>What’s a Diffusion Language Model (DLM)?</strong></h4> <p>Quick recap: all the trendy autoregressive language models (AR-LMs) these days — GPT-2, Llama, Gemini, ChatGPT, Claude, you name it — use the Transformer backbone for <strong>autoregressive</strong> (AR) decoding. That means they predict one token at a time, left-to-right.</p> <p>Diffusion Language Models (DLMs), on the other hand, work differently. Instead of going token by token, they <strong>iteratively refine</strong> and predict the <em>whole</em> sequence from a noisy starting point — following a <strong>non-autoregressive</strong> (NAR) decoding process.</p> <p>Here’s a (<em>very simplified</em>) way to picture the difference between the two paradigms, shown in the figure below:</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion_vs_ar-480.webp 480w,/assets/img/diffusionlm_blog/diffusion_vs_ar-800.webp 800w,/assets/img/diffusionlm_blog/diffusion_vs_ar-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/diffusion_vs_ar.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. AR-LM predicts the sequence token-by-token. Output tokens are used as input for the next token-prediction in left-to-right manner (top). DLM iteratively refines the entire output sequence from a noisy sample (bottom). </div> <p>Putting it in mathematical terms: Suppose we want to predict a sequence \(\mathbf{x} = \{x_1, x_2, \ldots, x_N\}.\) An AR-LM (autoregressive language model) with parameters \(\theta\) models the following distribution:</p> \[\begin{equation} \label{eq:AR-LM} P(\mathbf{x}; \theta) = \prod_{n=1}^{N} P(x_n \mid \mathbf{x}_{&lt;n}; \theta) \end{equation}\] <p>In contrast, DLMs take a <em>holistic</em> view of the entire sequence. They model a different kind of distribution — one that evolves over time \(t\) in a <strong>reverse diffusion process</strong> (<em>don’t worry, we’ll get into the details very soon</em>). Here, a larger \(t\) corresponds to a noisier version of the sequence — something closer to random Gaussian noise.<br> You can think of it like we start with a super messy paragraph and then <em>iteratively <a href="https://www.youtube.com/watch?v=AppsjTInqiw" rel="external nofollow noopener" target="_blank">CLEAN</a> it up</em> until it becomes the polished passage we actually want.</p> \[\begin{equation} \label{eq:DLM} \mathbf{x}_{t-1} \sim p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, t) \end{equation}\] <h4 id="why-do-we-need-dlms-or-nar-whats-wrong-with-ar"><strong>Why do we need DLMs (or NAR)? What’s wrong with AR?</strong></h4> <p>Autoregressive models are insanely successful these days — so why bother with another paradigm? Why is DLM a field worth looking into? Well, here are some arguments for you to consider:</p> <hr> <ul> <li> <strong>Inherent Disadvantages of AR-LMs</strong> <ul> <li> <p><strong>Error Propagation</strong>:<br> In autoregressive models, if you make a mistake when predicting the current token, tough luck — you can’t go back and fix it. Future predictions are based on that flawed token, causing errors to propagate and accumulate over time. This painful phenomenon is known as <a href="https://aclanthology.org/D18-1396/" rel="external nofollow noopener" target="_blank">error propagation</a>.</p> </li> <li> <p><strong>Indirect Generation Control</strong>:<br> Controlling AR generation is tricky. Most methods rely on heavy training or hacks during decoding — and honestly, they’re pretty inconvenient. For example, if you want to generate a passage of a certain length, you either have to train a separate length predictor or do fancy prompting. Other controls may rely on heuristics like <a href="https://arxiv.org/abs/1904.09751" rel="external nofollow noopener" target="_blank">top-k sampling</a>. And even then… there’s no guarantee it’ll work 😥.</p> </li> <li> <p><strong>Computational Constraints</strong>:<br> Token-by-token generation is slow because the model must wait for previous tokens to be fully decoded before predicting the next ones. Plus, the strict left-to-right setup limits tasks that require reverse reasoning — a problem known as the “<a href="https://arxiv.org/abs/2309.12288" rel="external nofollow noopener" target="_blank">Reversal Curse</a>”.</p> </li> </ul> </li> </ul> <hr> <ul> <li> <strong>(Potential) Advantages of DLMs</strong> <ul> <li> <p><strong>Non-Autoregressive (NAR) Generation</strong>:<br> Since sequences are generated holistically, the model can fix earlier mistakes as it refines the output — no more getting stuck with bad early guesses.</p> </li> <li> <p><strong>Controllability</strong>:<br> Diffusion models are naturally good at controllable generation! Using tricks like classifier-free guidance or classifier-based guidance (<a href="https://arxiv.org/abs/2105.05233" rel="external nofollow noopener" target="_blank">Prafulla et al., 2021</a>, <a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">Radford et al., 2021</a>), we can easily steer the output style. In DLMs, this can extend even further — allowing fine-grained control over length, specific text edits, infillings, and structural constraints like code and tables (<a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al., 2022</a>, <a href="https://arxiv.org/abs/2502.09992" rel="external nofollow noopener" target="_blank">Nie et al., 2025</a>).</p> </li> <li> <p><strong>Diversity</strong>:<br> Want different outputs? Just sample different initial noise — no fancy beam search or sampling needed 🎲.</p> </li> <li> <p><strong>(<em>Potential</em>) Speed Up</strong>:<br> Since generation doesn’t have to happen strictly token-by-token, there’s potential for faster, more parallelized decoding.</p> </li> </ul> </li> </ul> <hr> <h4 id="diffusion-model-recap"><strong>Diffusion Model Recap</strong></h4> <p>Diffusion models are very successful and widely adopted in computer vision tasks, such as image generation, super-resolution, and inpainting. The core idea of diffusion models is to learn a generative model by reversing a diffusion process that gradually adds noise to the data. Using the famous <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">DDPM</a> as an example, given a data sample from a real data distribution \(\mathbf{x}_0 \sim \mathcal{D}(x)\), we use a <strong>forward process</strong> to gradually perturb the data with small amounts of Gaussian noise over \(T\) steps:</p> <p>\(\begin{equation} q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \end{equation}\) \(\begin{equation} q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}) \end{equation}\)</p> <p>where \(\beta_t \in (0, 1)\) is a variance schedule that controls the amount of noise added at each step. As \(T \to \infty\), \(\mathbf{x}_T\) approaches a sample from a standard Gaussian distribution:</p> \[\begin{equation} \lim_{T \to \infty} \mathbf{x}_T \approx \mathcal{N}(0, \mathbf{I}) \end{equation}\] <p>The <strong>reverse process</strong> then learns to gradually denoise the data, starting from pure noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\) and working backwards, where \(\mu_\theta\) and \(\Sigma_\theta\) are learned by a fancy neural network model. Again, if you are not comfortable with these concepts, please refer to <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">Lilian’s amazing blog</a>.</p> \[\begin{equation} p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \end{equation}\] \[\begin{equation} p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)) \end{equation}\] <div class="row mt-2"> <div class="col-sm-8 col-md-6 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/DDPM-480.webp 480w,/assets/img/diffusionlm_blog/DDPM-800.webp 800w,/assets/img/diffusionlm_blog/DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/DDPM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Ho et al. 2020</a> with a few additional annotations) </div> <h4 id="whats-the-fundamental-challenge"><strong>What’s the Fundamental Challenge?</strong></h4> <p>Well, if Diffusion Models are so well-established and come with all these exciting perks, why aren’t they as trending in NLP as they are in computer vision? 👀</p> <p>Good question — and now we get to the fundamental challenge: there’s a big discrepancy between traditional <strong>continuous</strong> diffusion models (which crushed it in image generation — see Denoising Diffusion Probabilistic Models, <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Ho et al., 2020</a>) and the world of <strong>discrete text</strong>.</p> <p>Think about an <strong>image</strong>: it’s made of pixels, and each pixel has color values (like RGB) — basically numbers on a continuous scale. Adding “noise” is super intuitive: you just perturb those numbers a little, typically by adding random Gaussian noise. Gradually adding more and more noise smoothly transitions the image into random static. And the reverse process? Just train a model to predict and subtract that noise step-by-step, and voilà — the original image comes back.</p> <p>Now, consider <strong>text</strong>: language is made of <strong>discrete</strong> units — words or tokens picked from a finite vocabulary (“cat”, “dog”, “runs”, etc.). You can’t just add “0.1 Gaussian noise” to the word “cat” and expect to get something slightly fuzzier but still meaningful. Applying the same continuous noise idea directly <em>just doesn’t work</em>.</p> <p>This <strong>discrete nature</strong> of text is the core hurdle. The big question is:</p> <blockquote> <p>How do you define a “forward process” that gradually corrupts text into noise — and, critically, a “reverse process” that a model can learn to invert, step-by-step?</p> </blockquote> <p>Researchers have developed some clever workarounds to bridge this gap:</p> <hr> <p><strong>Operating on Continuous Variables</strong></p> <p>One approach is to not work with the words themselves, but with their embeddings, which are continuous. Traditional language models already produce well-constructed word embeddings and hidden layer outputs. We can leverage these representations to define a continuous forward process, where the model learns to predict the noise added to these continuous vectors at each step. This is similar to how diffusion models operate in the image domain — often working in the latent space of a VAE or similar architecture.</p> <ul> <li> <p><strong>Word Embedding (Token Level)</strong>:<br> Noise <em>can</em> be added to word embedding vectors — a technique used in models like <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Diffusion-LM</a> and the pre-trained DLM <a href="https://arxiv.org/abs/2212.11685" rel="external nofollow noopener" target="_blank">GENIE</a>. However, mapping potentially noisy embeddings back to specific discrete tokens at each step introduces its own complexities.</p> </li> <li> <p><strong>Higher-Level Latent Representations</strong>:<br> Works like <a href="https://arxiv.org/abs/2306.02531" rel="external nofollow noopener" target="_blank">PLANNER</a> and <a href="https://arxiv.org/pdf/2212.09462" rel="external nofollow noopener" target="_blank">LD4LG</a> operate on latent representations of <em>paragraphs</em> of text. But these representations can be pretty fragile — even small noise can cause abrupt semantic shifts during reverse diffusion. <strong>My own paper</strong> <a href="https://arxiv.org/abs/2412.11333" rel="external nofollow noopener" target="_blank">SLD</a> tackles this problem <em>cleverly</em> by introducing text-segmentation and improved representation learning techniques. Also worth checking out: Meta’s <a href="https://arxiv.org/pdf/2412.08821" rel="external nofollow noopener" target="_blank">Large Concept Model</a>, the existing pre-trained DLMs following this path.</p> </li> </ul> <hr> <p><strong>Discrete Diffusion over Tokens</strong></p> <p>Some <em>bold geniuses</em> thought: “Hey, if tokens are discrete, why not make the diffusion process discrete too?”</p> <p>And so — we now have discrete diffusion over categorical supports. <a href="https://arxiv.org/pdf/2102.05379" rel="external nofollow noopener" target="_blank">Eminel et al., 2021</a> extended diffusion to handle categorical data like text. Here, each token is represented as a probability mass vector over the vocabulary \(p \in \mathbb{R}^{V}\), where \(V\) is the vocabulary size. We use transition matrices \(\mathbf{Q}\) to model token-level transitions over timesteps, e.g.,</p> \[q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \text{Categorical}(\mathbf{x}_t; \mathbf{p} = \mathbf{x}_{t-1}\mathbf{Q}_t)\] <p>Models like <a href="https://arxiv.org/abs/2107.03006" rel="external nofollow noopener" target="_blank">D3PM</a> and <a href="https://arxiv.org/pdf/2310.16834" rel="external nofollow noopener" target="_blank">SEDD</a> (<em>ICML 2024 Best Paper</em> 🏆) follow this path. More commonly, text diffusion models define analogous discrete “noising” or “corruption” processes. Instead of adding mathematical noise, the forward process might involve:</p> <ul> <li> <p><strong>Masking</strong>:<br> Randomly replacing tokens with a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token, with the amount of masking increasing over diffusion steps.<br> (Example: <a href="https://arxiv.org/pdf/2502.09992" rel="external nofollow noopener" target="_blank">LLaDA</a> — an 8B-parameter pre-trained DLM that’s currently trending.)</p> </li> <li> <p><strong>Token Substitution</strong>:<br> Randomly replacing tokens with other tokens from the vocabulary. (Check out <a href="https://arxiv.org/abs/2305.14671" rel="external nofollow noopener" target="_blank">Zou et al., 2023</a>)</p> </li> <li> <p><strong>Hybrids</strong>:<br> Combining masking, substitution, and other discrete corruption methods. (Check out <a href="https://arxiv.org/abs/2209.00796" rel="external nofollow noopener" target="_blank">Yang et al., 2024</a>)</p> </li> </ul> <hr> <p>🔥🖼️👊 <strong>The Maverick — Text as Image</strong></p> <p><em>“Discrete text? What text? It’s an image!”</em> 🤣</p> <p>Instead of dealing with the discrete gap, <a href="https://arxiv.org/pdf/2304.12519" rel="external nofollow noopener" target="_blank">GlyphDiffusion</a> bypasses it entirely — by rendering the target text as glyph <strong>images</strong> containing visual language content. (<em>Yes, seriously. I like the idea very much. I personally wish you to check this out.</em>)</p> <hr> <p>In all these methods, the reverse process becomes about <strong>undoing</strong> specific types of corruption. For instance, the model learns to predict the original tokens at <code class="language-plaintext highlighter-rouge">[MASK]</code> positions, or correct randomly substituted tokens, gradually refining the sequence from a highly corrupted mess back into coherent text. So while the <em>core idea</em> of diffusion (iterative refinement from noise) stays the same, the <em>mechanisms</em> for forward (corruption) and reverse (denoising) processes have to be <strong>carefully adapted</strong> for the discrete world of language.</p> <hr> <p>Now, I’ll pick representative works from each paradigm to explain DLMs in more detail. <span style="color:blue">For each paradigm, I’ll introduce key papers, explain the mechanisms, and point you to off-the-shelf pre-trained models you can try out yourself!</span></p> <h4 id="embedding-level-diffusion--where-it-begins"><strong>Embedding-Level Diffusion — Where It Begins</strong></h4> <h5 id="1-token-level-embeddings"><strong>1. Token-Level Embeddings</strong></h5> <p><em>As far as I know</em>, <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Diffusion-LM</a> is probably the first influential work that kicked off the DLM era 🎉. Suppose we have a sequence of words: \(\mathbf{w} = \{w_1, w_2, \ldots, w_n\}\) An embedding function maps each word into a vector: \(Emb(w_i) \in \mathbb{R}^d\) Thus, the entire sequence is encoded into: \(\mathbf{x}_0 = Emb(\mathbf{w}) \in \mathbb{R}^{n \times d}\)</p> <p>Awesome! Now we have a <strong>continuous</strong> space where we can run good old conventional diffusion models.<br> (We use the typical simplified KL-divergence term from the evidence lower bound — which I won’t rehash here — to derive the loss.)</p> <p>Specifically, the training objective is:</p> \[\begin{equation} \mathcal{L}_{simple}(\mathbf{x}_0, \theta) = \sum_{t=1}^T \underset{q(\mathbf{x}_t \mid \mathbf{x}_0)}{\mathbb{E}} \left[ \|\mu_{\theta}(\mathbf{x}_t, t) - \hat{\mu}(\mathbf{x}_t, \mathbf{x}_0)\|^2 \right], \end{equation}\] <hr> <p>where \(\hat{\mu}(\mathbf{x}_t, \mathbf{x}_0)\) is the closed from Gaussian, the noised variable in the forward process. \(\mu_{\theta}(\mathbf{x}_t, t)\) is the predicted mean, computed by our trainable neural network, which is the diffusion model. But hold on — we can’t forget about converting embeddings <strong>back</strong> into discrete tokens! You might think: <em>“Easy, let’s just use another function to transform them back.”</em> And… you’d be mostly right. In <a href="https://github.com/XiangLi1999/Diffusion-LM" rel="external nofollow noopener" target="_blank">Li’s implementation</a>, they model these steps into the diffusion process as an <strong>extra timestep</strong>. As shown in the figure below (👀), the forward process includes an additional Markov transition to obtain embeddings:</p> \[q_{\phi}(\mathbf{x}_0 \mid \mathbf{w}) = \mathcal{N}(Emb(\mathbf{w}); \sigma_0^2 I)\] <p>Then, in the reverse process, you have an <strong>additional trainable rounding step</strong>, parameterized by:</p> \[p_{\theta}(\mathbf{w} \mid \mathbf{x}_0) = \prod_{i=1}^n p_{\theta}(w_i \mid x_i)\] <p>where each \(p_{\theta}(w_i \mid x_i)\) is a simple softmax distribution over the vocabulary.</p> <hr> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/diffusion-lm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. A graphical model representing the forward and reverse diffusion processes for Diffusion-LM. (Image source: <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al. 2022</a>) </div> <p>Then we can adjust the loss function accordingly. For end-to-end training, we arrive at the final loss function shown below. During inference (i.e., the reverse process), you sample a random embedding sequence containing \(n\) token embeddings — same as during training — and gradually remove the noise, step by step. You’re always predicting all \(n\) embeddings together, since the diffusion model expects fixed-shape inputs and outputs. (<em>Kind of wasteful if your sequence is shorter than \(n\), right? We’ll talk about that later.</em>)</p> \[\begin{equation} \mathcal{L}_{simple}^{e2e}(\mathbf{w}, \theta) = \underset{q_{\phi}(\mathbf{x}_{0:T} | \mathbf{w})}{\mathbb{E}} \left[\underbrace{\mathcal{L}_{simple}(\mathbf{x}_0)}_{\text{diffusion Loss}} + ||Emb(\mathbf{w}) - \overbrace{\mu_{\theta}(\mathbf{x}_1, 1)}^{\text{predicted }\mathbf{x}_0}||^2 - \underbrace{\log p_{\theta}(\mathbf{w} | \mathbf{x}_0)}_{\text{rounding}} \right] \end{equation}\] <p>So, everything seems super straightforward, right? Or… does it? Unfortunately, no 😅. The conversion between continuous embedding space and discrete tokens is actually <strong>non-trivial</strong> — and harder than you might think. <span style="color:red"> This rounding step is a key challenge in token embedding-level diffusion models. Discretization can introduce errors that accumulate across the diffusion process, since the embedding space isn’t uniformly filled with valid tokens. </span> <em>Well, isn’t this just the notorious data sparsity problem making a comeback?</em></p> <hr> <p>In the paper, there’s a whole section dedicated to techniques for reducing <strong>rounding error</strong> and producing better outputs. For instance:</p> <ul> <li>They use the <strong>reparameterization trick</strong> to ensure every term in the loss explicitly models \(\mathbf{x}_0\).</li> <li>They also introduce a <strong>clamping trick</strong>, which maps each predicted vector \(\mathbf{x}_t\) to its nearest word embedding in every reverse sampling step.</li> </ul> <p>Still, a lot of work remains to be done here if we want to really boost generation quality.</p> <hr> <p>Back to the model itself, with the diffusion pipeline, you can do the fancy conditioning and controlled generation during your inference now. For example, you could have a separate neural network classifier and a class condition \(\mathbf{c}\). During the backward process, you obtain the \(\mathbf{x}_{t-1}\) with respect to the posterior probability using the gradient update below.</p> \[\begin{equation} \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{c}) = \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}) + \underbrace{\nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{c} | \mathbf{x}_{t-1})}_{\text{classifier guidance}} \end{equation}\] <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-classifier-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/diffusion-lm-classifier.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. For controllable generation, we iteratively perform gradient updates on these continuous latents to optimize for fluency (parametrized by Diffusion-LM) and satisfy control requirements (parametrized by a classifier). (Image source: <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al. 2022</a>) </div> <p>The table below gives a demonstration of how Diffusion-LM outperforms traditional controlled generation paradigm (FUDGE, Fine-tuning) in review generation. The paper also provides a bunch of experiments on controlled generation — including syntax tree, length, part-of-speech, and more.</p> <table border="1" style="border-collapse: collapse; width: 100%;"> <thead> <tr> <th style="border: 1px solid #000; padding: 4px;">target semantic content</th> <th style="border: 1px solid #000; padding: 4px;">name : Travellers Rest Beefeater</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #000; padding: 4px;">FUDGE</td> <td style="border: 1px solid #000; padding: 4px;"> <span style="color:red;">Clowns near Clare Hall</span> in riverside is a French coffee shop rated 5 out of 5 </td> </tr> <tr> <td style="border: 1px solid #000; padding: 4px;">Diffusion-LM</td> <td style="border: 1px solid #000; padding: 4px;"> <span style="color:red;">Green Man</span> is an Italian pub located in the city centre near Café UNK. </td> </tr> <tr> <td style="border: 1px solid #000; padding: 4px;">FT</td> <td style="border: 1px solid #000; padding: 4px;"> <span style="color:green;">Travellers Rest Beefeater</span> is a reasonably priced restaurant that is family friendly. </td> </tr> </tbody> </table> <div class="caption"> Table 1. Semantic control outputs for generating reviews for the target "Travellers Rest Beefeater" across different models. Generations adheres to control is highlighted in green, violations are highlighted in red. (Table source: <a href="https://arxiv.org/abs/2205.14217" rel="external nofollow noopener" target="_blank">Li et al. 2022</a>) </div> <p>But I want to highlight <strong>infilling</strong> specifically, because it’s super neat. 🧩 In this setting, during inference, instead of denoising <em>all</em> embeddings, some context embeddings are <strong>given and fixed</strong>. For example: \(\mathbf{x}_t =\) <code class="language-plaintext highlighter-rouge">[w_1] [MASK] [w_2]</code>. The diffusion model is told to only generate the noisy token in the middle. This is done by masking the gradients of the fixed tokens — so they stay untouched — while still using them as <strong>context</strong> during the reverse sampling process. In other words, the fixed tokens act as <strong>classifier-free guidance</strong>. And <em>you</em>, my clever reader, have probably already realized: this setup makes it easy to model traditional sequence-to-sequence tasks — just give the input as the left context!</p> <hr> <p><a href="https://arxiv.org/pdf/2212.11685" rel="external nofollow noopener" target="_blank">GENIE</a> is (<em>again, as far as I know</em>) the <strong>first pre-trained DLM</strong> to follow this token embedding-level diffusion path. It uses a <strong>continuous paragraph denoising</strong> objective for pretraining.</p> <p>The idea:</p> <ul> <li>Apply a hybrid noising process to the original text — including token masking (like <code class="language-plaintext highlighter-rouge">[w_1] [MASK] [w_2]</code>) and forward diffusion sampling.</li> <li>Then train the model to recover the clean original text from the noisy version.</li> </ul> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GENIE-480.webp 480w,/assets/img/diffusionlm_blog/GENIE-800.webp 800w,/assets/img/diffusionlm_blog/GENIE-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/GENIE.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5. The framework of GENIE. Source sequence is encoded as the condition of the transformer DLM through cross attention. DLM restores the randomly initial Gaussian noise to the output text through the iterative denoising and grounding process (Image source: <a href="https://arxiv.org/pdf/2212.11685" rel="external nofollow noopener" target="_blank">Lin et al., 2022</a>) </div> <p>Notably, GENIE doesn’t use infilling as its default sequence-to-sequence generation method. Instead, it follows more of an <strong>Encoder-Decoder</strong> approach (yep, think BART or T5) — which is actually another form of <strong>classifier-free guidance</strong>. The input is fed to the diffusion model, a transformer in this case, as cross-attention targets. The equation below covers the key training step. They use a cross-attention transformer \(z_{\theta}\) to predict the mean of word-embeddings for the next timestep. \(\mathbf{H}_s = \{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n\}\) is the encoder output of the \(n\)-token long input, as the guidance.</p> \[\begin{equation} \mu_{\theta}^{t-1} = \frac{1}{\sqrt{\alpha_t}} \Biggl( \mathbf{x_t} \;-\; \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}\, z_{\theta}(\mathbf{x_t}, t, \mathbf{H}_s) \Biggr) \end{equation}\] <p>GENIE’s inference process is the same as Li’s work. Similar rounding techniques are applied here too: GENIE uses an efficient <strong>KNN (k-nearest neighbors) algorithm</strong> to retrieve the closest word embedding for each token during reverse sampling, then rounds to the nearest learned word embedding. This rounding step helps map noisy continuous vectors back to valid discrete tokens more effectively — though, as always, there’s still room for improvement!</p> <h5 id="2-higher-level-embeddings"><strong>2. Higher-Level Embeddings</strong></h5> <p>To address the <strong>rounding error</strong> problem when translating predicted word embeddings back to discrete tokens, researchers have explored bringing diffusion into a <strong>higher-level semantic latent space</strong> — like at the sentence or paragraph level. In this setup, the diffusion model doesn’t operate directly on word embeddings. Instead, it predicts a latent representation of an entire piece of text. Then, a separate <strong>autoregressive decoder</strong> is used to decode that latent into natural language. So the rounding is bypassed as we are not handling discrete text during the diffusion process. One representative example is the work by Lovelace et al., <strong>Latent Diffusion for Language Generation</strong> (<a href="https://arxiv.org/abs/2212.09462" rel="external nofollow noopener" target="_blank">LD4LG</a>). We’ll use this to walk through how the concept works.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LD4LG-480.webp 480w,/assets/img/diffusionlm_blog/LD4LG-800.webp 800w,/assets/img/diffusionlm_blog/LD4LG-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/LD4LG.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Overview of Latent Diffusion For Language Generation (Image source: <a href="https://arxiv.org/abs/2212.09462" rel="external nofollow noopener" target="_blank">Lovelace et al., 2023</a>) </div> <p>First, an encoder model (e.g., BART or T5 Encoder), denoted as \(Enc()\), is used to convert a piece of text \(\mathbf{w} = \{w_1, \ldots, w_n\}\) into hidden states: \(Enc(\mathbf{w}) \in \mathbb{R}^{n \times h_{lm}}\) where \(h_{lm}\) is the hidden size of the encoder (typically 768 or 1024, etc.).</p> <p>Notice that \(n\) here represents the <strong>variable token length</strong> of the text, but diffusion models usually require a <strong>fixed-shape</strong> representation. In token-level embedding diffusion, this imposes a hard constraint on the context window size. For higher-level embeddings, we use an additional <strong>pooling/compression unit</strong> \(f()\) to project the hidden states into a fixed-length latent: \(\mathbf{z} = f(Enc(\mathbf{w})) \in \mathbb{R}^{k \times h_{rep}}\) where \(k\) and \(h_{rep}\) are hyperparameters defining the shape of the latent space. Typically, you want \(k &lt; n\) and \(h_{rep} \ll h_{lm}\) — because you don’t want a sparse, oversized latent. A more compact latent space helps the diffusion model learn the distribution more effectively.</p> <hr> <p>Then, the diffusion model \(R(\cdot \mid \theta)\) (usually a <strong>diffusion transformer</strong>, like <a href="https://arxiv.org/abs/2212.09748" rel="external nofollow noopener" target="_blank">DiT</a>), is deployed to learn how to recover \(\mathbf{z}_0\) from its noised version \(\mathbf{z}_t\) over the forward diffusion process — as usual.</p> \[\begin{equation} \mathcal{L}(\theta_{R}) = \sum_{t=1}^T \underset{q(\mathbf{z}_{t} | \mathbf{z}_0)} {\mathbb{E}} \left\| \hat{\mathbf{z}}_{t} - \mathbf{z}_t \right\|^2_2 \end{equation}\] <p>For inference, you start by sampling a latent representation from Gaussian noise, then run the reverse process as usual \(\hat{\mathbf{z}}_{t-1} = R(\hat{\mathbf{z}}_t, t; \theta_R)\) Conditioning (e.g., class labels, prompts) can also be added to both the forward and reverse processes — just like in other diffusion models. Next, we apply a <strong>reconstruction unit</strong> \(g()\) to project the denoised latent state back into the hidden dimension of the decoder LM. Then, the decoder generates text as follows: \(Dec(g(\mathbf{z}_0))\) Since we’re doing diffusion over the <strong>continuous latent space of high-level semantics</strong>, we effectively bypass the whole <strong>rounding</strong> phase from token-level embedding models.</p> <hr> <p><strong>BUT</strong>, is it that simple? Actually… no ☹️. <span style="color:red"> It’s non-trivial to find a good latent representation for text — let alone generate one from noisy examples. </span> For example, take the sentence: <code class="language-plaintext highlighter-rouge">"It is sunny today"</code> Now, if I perturb its latent representation just a bit, what should that mean? Well, this is <em>semantic</em> diffusion, right? So a small change should preserve the <strong>meaning</strong>, not just the surface form. We’d hope to get something like: <code class="language-plaintext highlighter-rouge">"Today is sunny"</code> or <code class="language-plaintext highlighter-rouge">"Today has sun"</code> Not: <code class="language-plaintext highlighter-rouge">"It isn't sunny today"</code> or <code class="language-plaintext highlighter-rouge">"It is savvy tody"</code> 🙃 The latter examples may look similar in terms of characters or token embeddings, but they’re clearly <strong>not</strong> semantically close.</p> <p>And that’s the crux of the problem: the <strong>meaning</strong> of even a short paragraph is super rich and subtle. How are we supposed to regularize a latent space that captures <em>that</em>? This poses a major challenge for high-level semantic diffusion — especially when it comes to <strong>long-form generation</strong>, as it usually packs more and complex meanings.</p> <p>So… back to square one: <strong>What’s the definition of a good latent representation for high-level semantic diffusion?</strong> <a href="https://arxiv.org/pdf/2306.02531" rel="external nofollow noopener" target="_blank">Zhang et al., 2024</a> gives a formal definition of this, describing three key desiderata:</p> <hr> <p><strong>1. Low Conversion Error</strong><br> Given a piece of text \(\mathbf{w}\), we encode it into latent representations and decode it back: \(\tilde{\mathbf{w}} = Dec(g(f(Enc(\mathbf{w}))))\) The difference between the original \(\mathbf{w}\) and the reconstructed \(\tilde{\mathbf{w}}\) should be minimal — ideally none. However, this is <strong>hard</strong> when \(\mathbf{w}\) is long. Remember, \(k\) and \(h_{rep}\) are fixed hyperparameters. So during projection, information gets compressed — and longer sequences suffer more loss. (<em>Makes sense, right? The longer the sequence, the more stuff you have to cram into a fixed-size box!</em> 📦)</p> <hr> <p><strong>2. Local Smoothness</strong><br> Imagine someone stuttering or dropping a few minor words while speaking — you can usually still understand them. Similarly, given a piece of text \(\mathbf{w}\) and a slightly varied version \(\mathbf{w'}\), their encoded latent representations should be <strong>close</strong> to each other: \(\mathbf{z}_{w} \approx \mathbf{z}_{w'}\) This ensures the latent space is locally smooth, tolerant to small, surface-level changes without drastic semantic shifts.</p> <hr> <p><strong>3. Distributional Smoothness</strong><br> In the high-level latent space, we want meanings of paragraphs to be <strong>smoothly distributed</strong>. That is:</p> <ul> <li>A piece of text \(\mathbf{w}\) and its paraphrases should have nearby latent vectors.</li> <li>Small perturbations in latent space should preserve meaning.</li> <li>Texts with very different meanings should be <strong>far apart</strong> in latent space.</li> </ul> <p>Sounds good — but <strong>super hard</strong> in practice, especially for long-form text! Longer sequences carry multiple complex ideas, making the latent space messy and harder to regularize. If you increase the size of the latent space to capture all that complexity, the diffusion model faces another challenge: learning a distribution \(p(\mathbf{z})\) that is highly <strong>multimodal</strong> or has a large <strong>Lipschitz constant</strong> — meaning the density function can change very abruptly, which is nasty for diffusion models to handle.</p> <hr> <p>So the truth is: without proper regularization, the learned latent distribution can become <strong>fragile</strong> — small perturbations might cause abrupt semantic shifts, making life hard for the diffusion model and <strong>catastrophically corrupting</strong> the quality of the decoded text. Yikes.</p> <p>So… how do we fix this?</p> <hr> <p><a href="https://arxiv.org/pdf/2412.11333" rel="external nofollow noopener" target="_blank">Zhu et al., 2024</a> (<em>yeah that’s me 🤪</em>) proposes <strong>Segment-Level Diffusion (SLD)</strong>. Inspired by the concept of <strong>patching</strong> in image generation, we “patch” the text into coherent segments — like individual <strong>sentences</strong>, <strong>dialogue turns</strong>, or <strong>utterances</strong>. This gives us much better control over both the <strong>length</strong> of each segment and the <strong>semantic scope</strong> of each latent representation.</p> <hr> <p>We further regularise the latent representations by doing additional training for representation learning, using <strong>contrastive learning</strong>, and <strong>adversial noise preturbation</strong>, ensuring local and distributional smoothness.</p> <p>These tricks ensure both <strong>local</strong> and <strong>distributional smoothness</strong>, just like we talked about earlier. The diffusion model then learns to predict <strong>multiple latent representations</strong>, with <strong>one-to-one correspondence</strong> to each segment. Each segment’s latent can then be <strong>decoded independently — and in parallel!</strong> That means better generation quality <em>and (maybe)</em> faster inference (<em>in theory, we will talk about this in the end</em>). 💨</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/SLD-480.webp 480w,/assets/img/diffusionlm_blog/SLD-800.webp 800w,/assets/img/diffusionlm_blog/SLD-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/SLD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Overview of the training pipeline of SLD. In the first stage, gold output is divided into segments. In the second stage, we use contrastive and adversarial learning to ensure latent representations are robust to drastic semantic changes. Finally, we train a diffusion model as an inherent semantic planner conditioned on given inputs. (Image source: <a href="https://arxiv.org/pdf/2412.11333" rel="external nofollow noopener" target="_blank">Zhu et al., 2024</a>) </div> <p>As demonstrate in the figure above, our SLD pipeline contains three major components, segmentation, representation learning and diffusion LM training. Blue units are trainable neural networks. On the top left, a paragraph of desired output text, the storylines, are <strong>segmented into sentences</strong>. These sentences are encoded and projected to a compact latent space. To further regularise the latent representation, as shown in top right, we do <strong>contrastive learning</strong>. We generate paraphrases as positive samples pulling them together, and randomly sample out-of-domain text as negative samples pushing them apart. In addition, we add some <strong>adversarial (worst case) noise</strong> to perturb the representations, training decoding units to be more robust. The bottom part describes the diffusion model training, we use cross-attention transformer conditioning on inputs and ensures <strong>1-to-1</strong> correspondence between latent representations and sentences over this process. The representations of all segments are decoded <strong>in-parallel</strong> in the end.</p> <p>And you can see the importance of representation learning in the visualization below. Before the representation learning, the desired cluster of sentences (ROCStories) are not very distinguishable from other texts (CNN/Daily Mail), which makes the model susceptible to abrupt semantic changes during diffusion process. With regularization, it’s much better. Adversial noise is to ensure we enhance the generation quality even better.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/SLD_REP-480.webp 480w,/assets/img/diffusionlm_blog/SLD_REP-800.webp 800w,/assets/img/diffusionlm_blog/SLD_REP-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/SLD_REP.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of PCA 2D projections of latent representations for sampled segmented sentences from ROCStories (Blue), their paraphrases (Green), and out-of-domain (OOD) sentences sampled from CNN/Daily Mail (Orange) under three training paradigms: Vanilla training, Noise Robust training, and Noise Robust + Contrastive learning. The red trajectory illustrates the denoising path of the sentence 'David noticed he had put on a lot of weight recently.' The trajectory is annotated with noise ratios, where 1.0 (Lighter Red) represents pure Gaussian noise and 0.0 (Darker Red) indicates no noise. (Image source: <a href="https://arxiv.org/pdf/2412.11333" rel="external nofollow noopener" target="_blank">Zhu et al., 2024</a>) </div> <p>A contemporaneous work from Meta, <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/" rel="external nofollow noopener" target="_blank">Large Concept Model</a>, uses a similar paradigm. They perform diffusion over <strong>concepts</strong> — which is pretty much the same idea as <strong>segments</strong> in my work. Definitely check out their paper! They provide a model pre-trained on way more data than I had access to. In their work, they use a multimodal and multilingual encoder <a href="https://arxiv.org/abs/2308.11466" rel="external nofollow noopener" target="_blank">SONAR</a> (<em>definitely more powerful than the Flan-T5 encoder we were using</em>) to generate concept embeddings.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LCM-480.webp 480w,/assets/img/diffusionlm_blog/LCM-800.webp 800w,/assets/img/diffusionlm_blog/LCM-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/LCM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Left: visualization of reasoning in an embedding space of concepts (task of summarization). Right: fundamental architecture of an Large Concept Model (LCM). Note that the LCM is a diffusion model! (Image source: <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/" rel="external nofollow noopener" target="_blank"> LCM Team 2024</a>) </div> <h4 id="discrete-diffusion-over-tokens"><strong>Discrete Diffusion over Tokens</strong></h4> <p>In this section, we’ll dive into how <strong>diffusion over discrete tokens</strong> is done — focusing on the <strong>Masked Diffusion Model (MDM)</strong> as introduced by <a href="https://arxiv.org/abs/2406.07524v2" rel="external nofollow noopener" target="_blank">Sahoo et al.</a> in <em>Masked Diffusion Language Models</em>.</p> <p>This will be our main example, but I also encourage you to check out:</p> <ul> <li><a href="https://arxiv.org/abs/2107.03006" rel="external nofollow noopener" target="_blank">D3PM</a></li> <li> <a href="https://arxiv.org/pdf/2310.16834" rel="external nofollow noopener" target="_blank">SEDD</a> (<em>ICML 2024 Best Paper!</em> 🏆)</li> </ul> <p>However, fair warning: both are a bit mathematically dense and terse for a light intro blog like this one — so we’ll keep things digestible here. 🫠</p> <hr> <p><a href="https://arxiv.org/abs/2107.03006" rel="external nofollow noopener" target="_blank">D3PM</a> introduces a <strong>Markov forward process</strong> over tokens, using a sequence of <strong>categorical distributions</strong> constructed through multiplication of transition matrices \(\mathbf{Q}_t\) across \(T\) discrete timesteps. Concretely, we have a series of matrix multiplications:</p> \[\begin{equation} \mathbf{x}_T = \mathbf{Q}_{T-1} \cdot \mathbf{Q}_{T-2} \cdots \mathbf{Q}_1 \cdot \mathbf{x}_0 \end{equation}\] <p>This gradually transforms the initial sequence \(\mathbf{x}_0\) into a <strong>stationary distribution</strong> — i.e., full corruption.</p> <p>In their work, tokens are represented as: \(\mathbf{x} \in \mathcal{V}\) where \(\mathcal{V}\) is the set of all one-hot vectors of the vocabulary, and \(|\mathcal{V}| = K\).</p> <p>They define \(\text{Cat}(\cdot; \boldsymbol{\pi})\) as a <strong>categorical distribution</strong> over \(K\) token classes, with class probabilities given by \(\boldsymbol{\pi} \in \Delta^K\) — the <strong>K-simplex</strong> (i.e., the space of valid probability vectors over \(K\) classes). They also define a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token: \(\mathbf{m} \in \mathcal{V}\)</p> <p>During the <strong>forward process</strong>, they interpolate discrete diffusion by gradually converting \(\mathbf{x}\) into increasingly noisy variables \(\mathbf{z}_t\). The marginal distribution of \(\mathbf{z}_t\) conditioned on the original \(\mathbf{x}\) is:</p> \[\begin{equation} q(\mathbf{z}_t \mid \mathbf{x}) = \text{Cat}(\mathbf{z}_t; \alpha_t \mathbf{x} + (1 - \alpha_t)\boldsymbol{\pi}) \end{equation}\] <p>Here, \(\alpha_t\) is a scalar from the <strong>noise schedule</strong>, just like in standard diffusion models.</p> <hr> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/MDLM-480.webp 480w,/assets/img/diffusionlm_blog/MDLM-800.webp 800w,/assets/img/diffusionlm_blog/MDLM-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/MDLM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> (Left) Masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling.(Bottom Right) Perplexity (PPL) on One Billion Words benchmark. (Image source: <a href="https://arxiv.org/abs/2406.07524v2" rel="external nofollow noopener" target="_blank"> Sahoo et al., 2024</a>) </div> <p>In the <a href="https://arxiv.org/abs/2406.07524v2" rel="external nofollow noopener" target="_blank">Sahoo et al.</a>’s <strong>masked diffusion</strong> variant, they set: \(\boldsymbol{\pi} = \mathbf{m}\)</p> <p>This means that at each timestep \(t\), the input token \(\mathbf{x}\) has a chance of being replaced by the special <code class="language-plaintext highlighter-rouge">[MASK]</code> token \(\mathbf{m}\). Once a token is masked — i.e., it transitions to \(\mathbf{m}\) at some time \(t\) — it <strong>stays masked</strong> for all subsequent timesteps. 🫥 (No going back from <code class="language-plaintext highlighter-rouge">[MASK]</code> — it’s a one-way trip.)</p> \[\begin{equation} q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x}) = \begin{cases} \text{Cat}(\mathbf{z}_s; \mathbf{z}_t), &amp; \mathbf{z}_t \neq \mathbf{m},\\ \text{Cat}\left( \mathbf{z}_s; \frac{(1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t)\mathbf{x}}{1 - \alpha_t} \right), &amp; \mathbf{z}_t = \mathbf{m}. \end{cases} \end{equation}\] <p>So, consequently, for the reverse diffusion process, we train a network to do \(p_{\theta} (\mathbf{z}_s | \mathbf{z}_t)\) to convert masked tokens back to concrete tokens.</p> \[\begin{equation} p_\theta(\mathbf{z}_s \mid \mathbf{z}_t) = q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x} = \mathbf{x}_\theta(\mathbf{z}_t, t)) = \begin{cases} \text{Cat}(\mathbf{z}_s; \mathbf{z}_t), &amp; \mathbf{z}_t \neq \mathbf{m}, \\ \text{Cat}\left(\mathbf{z}_s; \frac{(1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t)\mathbf{x}_\theta(\mathbf{z}_t, t)}{1 - \alpha_t} \right), &amp; \mathbf{z}_t = \mathbf{m}. \end{cases} \end{equation}\] <p>Great, right? In this case, they’ve successfully extended diffusion from the <strong>continuous</strong> into the <strong>discrete</strong> domain. 🎉 I’ve omitted a few technical details — but you can check those out in their excellent <a href="https://s-sahoo.com/mdlm/" rel="external nofollow noopener" target="_blank">blog</a> or the full <a href="https://arxiv.org/abs/2406.07524v2" rel="external nofollow noopener" target="_blank">paper</a>.</p> <hr> <p>That said, note that the current design has some limitations. One issue is during <strong>decoding</strong>: Once a token is unmasked, it stays fixed. This isn’t ideal — especially in the <strong>early stages of reverse diffusion</strong>, when the paragraph is still mostly noise and the few decoded tokens are likely <em>not</em> optimal. But since they’re locked in place, the model must condition future generations on possibly bad guesses — causing the dreaded <strong>error propagation</strong> problem all over again. 😞</p> <hr> <p><a href="https://arxiv.org/abs/2302.05737" rel="external nofollow noopener" target="_blank">Lin et al.</a>, from <a href="https://ikekonglp.github.io/" rel="external nofollow noopener" target="_blank">Lingpeng Kong’s group</a> (<em>I really like their work!</em>) propose a fix for this using a <strong>routing</strong> mechanism. The idea? Even during decoding, a previously decoded token can be <strong>remasked</strong> (i.e., turned back into <code class="language-plaintext highlighter-rouge">[MASK]</code>) if the model’s confidence in that token is low. This allows the model to revisit and revise its decisions — bringing in a form of <strong>iterative refinement</strong> that’s more faithful to the spirit of diffusion.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LLaDA.svg" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/LLaDA.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> (a) Pre-training. LLaDA is trained on text with random masks applied independently to all tokens at the same ratio t ∼ U[0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion process from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies. (Image source: <a href="https://arxiv.org/abs/2502.09992" rel="external nofollow noopener" target="_blank"> Nie et al., 2025</a>) </div> <p>Finally, <a href="https://arxiv.org/abs/2502.09992" rel="external nofollow noopener" target="_blank"><strong>LLaDA 8B</strong></a> combines <strong>both</strong> techniques mentioned above — Masked Diffusion Language Modeling (<strong>MDLM</strong>) and <strong>routing</strong> — demonstrating the real potential of MDM as a new paradigm for <strong>pre-trained language models</strong>. The results? LLaDA achieves <strong>on-par</strong> — and in some cases <strong>superior</strong> — generation quality compared to autoregressive LLMs of the same size. Amazing, isn’t it? 🤯</p> <p>Try <a href="https://huggingface.co/spaces/multimodalart/LLaDA" rel="external nofollow noopener" target="_blank">it</a> out yourself!</p> <h4 id="text-in-image-diffusion-brain-teaser"><strong>Text-in-Image Diffusion? (Brain-teaser)</strong></h4> <p>Think <strong>computer vision</strong> for a moment! 👀 These days, NLP and CV have been borrowing ideas from each other all the time — just look at <a href="https://arxiv.org/abs/2404.02905" rel="external nofollow noopener" target="_blank">Tian et al., 2024</a>’s VAR, which brings autoregressive generation into image synthesis. So hey, if we’re already using <strong>diffusion</strong> (the most trending paradigm in CV), why not push the crossover even further? I’ll skip over some amazing CV works like <a href="https://github.com/deep-floyd/IF" rel="external nofollow noopener" target="_blank">DeepFloyd IF</a> for the sake of conciseness. Instead, here’s a fun little brain-teaser to wrap things up: <a href="https://arxiv.org/pdf/2304.12519v2" rel="external nofollow noopener" target="_blank"><strong>GlyphDiffusion</strong></a>.</p> <hr> <p>The key idea is <em>wild</em> but clever:</p> <ul> <li>Render the target text as a <strong>glyph image</strong> — that is, an actual visual representation of the characters —</li> <li>and treat conditional text generation as a <strong>glyph image generation</strong> task.</li> </ul> <p>Now that you’re working in the <strong>image domain</strong>, you can naturally apply all your favorite continuous diffusion tricks — <strong>no discrete token rounding or masking needed</strong>! <em>Let’s not worry (yet) about whether this is scalable or practical.</em> At the very least… it’s fun. 😄</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GlyphDiffusion-480.webp 480w,/assets/img/diffusionlm_blog/GlyphDiffusion-800.webp 800w,/assets/img/diffusionlm_blog/GlyphDiffusion-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/diffusionlm_blog/GlyphDiffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> GlyphDiffusion generate patches of image containing visual language contents. (Image source: <a href="https://arxiv.org/abs/2304.12519v2" rel="external nofollow noopener" target="_blank"> Li et al., 2023</a>) </div> <h4 id="challenges-and-opportunities"><strong>Challenges and Opportunities</strong></h4> <p>You might be wondering:</p> <blockquote> <p><em>“Hey, after reading through this blog, DLMs sound pretty well-established — we’ve even got pre-trained models already! So what’s stopping us from using them more widely?”</em></p> </blockquote> <p>Great question. The short answer? <strong>Speed</strong>.</p> <hr> <p>The major challenge for DLMs right now is <strong>inference efficiency</strong> and <strong>sampling cost</strong>. Yes, we’ve talked about how non-autoregressive (NAR) generation could <em>potentially</em> be faster than traditional AR methods — but… we’re not quite there yet. Autoregressive models have tricks like <strong>KV caching</strong>, which significantly boost decoding speed. Unfortunately, these acceleration methods aren’t applicable (yet) for NAR models like DLMs. Right now, naive DLM implementations are about <strong>50× slower</strong> than AR baselines. 🐢</p> <hr> <p><strong>But!</strong> Don’t be discouraged — we’re making steady progress. There’s growing work adapting <strong>acceleration techniques</strong> to the diffusion domain, including:</p> <ul> <li><a href="https://openai.com/index/simplifying-stabilizing-and-scaling-continuous-time-consistency-models/" rel="external nofollow noopener" target="_blank"><strong>Consistency Models</strong> (OpenAI)</a></li> <li><a href="https://arxiv.org/abs/2305.04465" rel="external nofollow noopener" target="_blank"><strong>Adaptive Decay Sampling</strong> (Tang et al., 2023)</a></li> <li> <a href="https://arxiv.org/abs/2402.07754" rel="external nofollow noopener" target="_blank"><strong>Diffusion of Thoughts</strong> (Ye et al., 2024)</a>, where they use <strong>ODE solvers</strong> to speed up decoding</li> </ul> <p>And hey, some people have <em>already</em> pulled it off! Check out Inception Labs’ <a href="https://chat.inceptionlabs.ai/" rel="external nofollow noopener" target="_blank">Mercury Coder</a> — the first <strong>fast commercial DLM</strong> out in the wild!</p> <hr> <p>But speed isn’t everything. There are <strong>so many opportunities</strong> with DLMs that we’ve only just begun to explore:</p> <ul> <li> <p>Since DLMs generate entire sequences holistically, could this change <strong>how we do reasoning</strong>?<br> Maybe it helps avoid bad intermediate steps and gives better <strong>chain-of-thought</strong> answers.<br> See: <a href="https://arxiv.org/abs/2402.07754" rel="external nofollow noopener" target="_blank">Diffusion-of-Thought</a>, <a href="https://hkunlp.github.io/blog/2025/dream/" rel="external nofollow noopener" target="_blank">Dream 7B</a></p> </li> <li> <p>DLMs are great at <strong>fine-grained control</strong>, like <strong>in-filling</strong>, making them ideal for generating structured outputs with constraints:<br> tables, code, logical forms, and more.<br> Check out: <a href="https://arxiv.org/pdf/2410.20626" rel="external nofollow noopener" target="_blank">TabDiff</a>, <a href="https://arxiv.org/abs/2407.02549" rel="external nofollow noopener" target="_blank">Mario et al., 2024</a></p> </li> <li> <p>The stochasticity of diffusion makes it a natural fit for <strong>data augmentation</strong> — especially useful for low-resource settings. <a href="https://aclanthology.org/2024.emnlp-main.109.pdf" rel="external nofollow noopener" target="_blank">Chen et al., 2024</a> used it to improve <strong>low-resource sentiment classification</strong>.</p> </li> <li> <p>We can also think about how would this change the uncertainty estimation? As you can do sampling more naturally, maybe shed lights on self-consistency and other confidence related decoding (<strong>remasking</strong>).</p> </li> <li> <p>And of course… <strong>multi-modal dreams</strong>!<br> Can we use one model to generate <strong>images and text</strong> together?<br> <a href="https://ai.meta.com/research/publications/transfusion-predict-the-next-token-and-diffuse-images-with-one-multi-modal-model/" rel="external nofollow noopener" target="_blank">Meta’s Transfusion</a> is already showing promising results.</p> <p><em>Though… technically they use AR for both images annd text.</em><br> <em>So when we are doing this the other way around, maybe we should rename it: <strong>FusionTrans™</strong> or <strong>DiffFormer™</strong></em> 😎</p> </li> </ul> <hr> <p>Oh — and if you’re curious to keep up with this fast-growing area, shout out to the people who keep up-to-date list of all known <a href="https://github.com/bansky-cl/diffusion-nlp-paper-arxiv" rel="external nofollow noopener" target="_blank">DLM papers on GitHub</a>. Highly recommended if you want to go down the rabbit hole. 🐇📚</p> <h5 id="epilogue"><strong>Epilogue</strong></h5> <p>I’ll end this introductory blog <strong>without a conclusion</strong> — because the <strong>Era of DLM</strong> has only just begun. 🚀</p> <p>I hope this blog helped you understand DLMs a little better, and maybe even sparked some ideas if you’re thinking about doing DLM-related research yourself! (<em>Oh gosh, I hope I didn’t make too many dad jokes or use too many emojis.</em> 😅) If you liked this post, I might write more blogs in the future to dive deeper into specific aspects.</p> <hr> <p>If you found this blog helpful and want to do me a favour, you can cite my <a href="https://arxiv.org/abs/2412.11333" rel="external nofollow noopener" target="_blank">Segment-Level Diffusion</a> paper — I’ve incorporated most of this blog’s content into the related work section there. Also, feel free to leave a comment if you have any suggestions for improving the blog. I’d love to hear from you! 🙏</p> <p>Until next time…</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhu2025segmentleveldiffusionframeworkcontrollable</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Xiaochen Zhu and Georgi Karadzhov and Chenxi Whitehouse and Andreas Vlachos}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2412.11333}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
      <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.11333}</span><span class="p">,</span> 
<span class="p">}</span>
</code></pre></div></div> <hr> <p><strong>Update 24/05/2025</strong></p> <ol> <li>Check out <a href="https://deepmind.google/models/gemini-diffusion/" rel="external nofollow noopener" target="_blank">Gemini Diffusion Model</a>! Seems like this field is getting more and more attention!</li> <li>Our paper <a href="https://arxiv.org/abs/2412.11333" rel="external nofollow noopener" target="_blank">Segment-Level</a> Diffusion got accepted into <strong>ACL 2025 Main</strong>! See you in Vienna if you want to ask me anything!</li> <li>Special thanks to my amazing supervisor <a href="https://andreasvlachos.github.io/" rel="external nofollow noopener" target="_blank">Prof. Andreas Vlachos</a> for helping me in the project, and proof reading this blog. Shout out to my lab mates <a href="https://suchirsalhan.github.io/" rel="external nofollow noopener" target="_blank">Suchir</a> and <a href="https://www.cst.cam.ac.uk/people/zg258" rel="external nofollow noopener" target="_blank">Zeb</a> for encouraging me writing this blog.</li> </ol> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"SpaceHunterInf/SpaceHunterInf.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xiaochen Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"cv",description:"Here is my CV, updated in March 2025.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-what-are-diffusion-language-models",title:"What are Diffusion Language Models?",description:"A gentle, in-depth introduction of existing diffusion language models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion-language-models/"}},{id:"post-a-post-with-image-galleries",title:"a post with image galleries",description:"this is what included image galleries could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/photo-gallery/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/SpaceHunterInf","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/neo-zhu-01270824b","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=1tkzwd4AAAAJ","_blank")}},{id:"social-x",title:"X",section:"Socials",handler:()=>{window.open("https://twitter.com/ZhuNeo13294","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>