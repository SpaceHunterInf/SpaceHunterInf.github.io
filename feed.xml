<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://spacehunterinf.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://spacehunterinf.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-29T15:44:03+00:00</updated><id>https://spacehunterinf.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Xiaochen Zhu.. </subtitle><entry><title type="html">What are Diffusion Language Models?</title><link href="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/" rel="alternate" type="text/html" title="What are Diffusion Language Models?"/><published>2025-04-14T11:59:00+00:00</published><updated>2025-04-14T11:59:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2025/diffusion-language-models</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/"><![CDATA[<h4 id="preface"><strong>Preface</strong></h4> <p><a href="https://www.youtube.com/watch?v=X0Jti9F-oQA">Dear Reader</a>, I feel like writing this blog for a long time. Diffusion model for language generation is an exciting and emerging field that receives increasing attention. However, up until the moment I am writing this blog, there isn‚Äôt a comprehensive guide/intro covering such topics for the members of NLP/ML community who wish to conduct research in this area, prospectively. In this blog, we will walk through the history of diffusion language model, different paradigms in implementation, possible future research directions and applications (<em>also a few of my personal opinion, which might be biased, in italics</em>). The blog will be updated constantly, and I will probably turn this into a survey paper when the time is right.</p> <p>This blog targets audience who already has sufficient knowledge in Diffusion Models, and traditional autoregressive LLMs. If you don‚Äôt, no worries, here are resources I found extremely helpful.</p> <ul> <li><strong>For Diffusion Models</strong>: <ul> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a> by Lilian Weng <em>Strongly recommended</em></li> <li><a href="https://arxiv.org/abs/2208.11970">Understanding Diffusion Models: A Unified Perspective</a> by Calvin Luo</li> </ul> </li> <li><strong>For Autoregressive LLMs</strong>: <ul> <li><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&amp;list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&amp;index=8">Stanford CS224N</a> <em>GOAT course</em></li> </ul> </li> </ul> <h4 id="whats-diffusion-language-model-dlm"><strong>What‚Äôs diffusion language model (DLM)?</strong></h4> <p>A brief recap on all the trending autoregressive language model (AR-LM) nowadays, from GPT-2, Llama, to Gemini, ChatGPT, and Claude, these models are using transformer as the common backbone for <strong>auoregressive</strong> decoding (AR), that is predicting token by token in an left-to-right fashion. By contrast, diffusion language model (DLM) is iteratively refines and predicts the entire sequence from a sampled noisy input as <strong>non-autoregressive</strong> decoding (NAR). A straightforward (<em>simplified</em>) demonstration of the difference between two paradigms can be shown in the pipeline figure below.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion_vs_ar-480.webp 480w,/assets/img/diffusionlm_blog/diffusion_vs_ar-800.webp 800w,/assets/img/diffusionlm_blog/diffusion_vs_ar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/diffusion_vs_ar.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. AR-LM predicted the sequence token-by-token. Output tokens are used as input for the next token-prediction in left-to-right manner (top). DLM iteratively refines the entire output sequence from a noisy sample (bottom). </div> <p>Putting it into mathematical term is, given a sequence we want to predict, \(\mathbf{x} = \{x_1, x_2, \ldots x_N\}\) , AR-LM with parameter \(\theta\) models the following distribution. \(\begin{equation} \label{eq:AR-LM} P(\mathbf{x}; \theta) = \prod_{t=1}^{N} P(x_n | \mathbf{x}_{&lt;n}; \theta) \end{equation}\)</p> <p>Where DLM take a holistic view of the entire sequence, it models the following distribution, where \(t\) is the timestep in <strong>reverse diffusion process</strong> (<em>we will get to the details very soon</em>). A larger \(t\) means noiser sequence, closer to sampled from a standard gaussian noise. Consider we are having a very messy paragraph in the beginning and iteratively refines into the passage we want.</p> \[\begin{equation} \label{eq:DLM} \mathbf{x}_{t-1} \sim p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, t) \end{equation}\] <h4 id="why-we-need-dlm-or-nar-whats-wrong-with-ar"><strong>Why we need DLM (or NAR)? What‚Äôs wrong with AR?</strong></h4> <p>Autoregressive model is extremely successful nowadays, so why do we still need another paradigm? What makes DLM a field worth looking into? Well here are some arguments for you to consider.</p> <ul> <li><strong>Inherent Disadvantage of AR-LM</strong> <ul> <li><strong>Error Propagation</strong> For autoregressive models, if you made mistake in predicting the current token, there is no chance for going back for revision. Future predictions are always based on this flawed prediction, propagating and accumulating such errors. We call such struggle as <a href="https://aclanthology.org/D18-1396/">error propagation</a>.</li> <li><strong>Indirect Generation Control</strong> Current strategies for AR controlled generation depends on extensive training or hacks in decoding strategy. Such techniques are often indirect and inconvienient for users to control the generation. For example, if you want to generate a passage with certain length, you need to train a length predictor or use heuristics like <a href="https://arxiv.org/abs/1904.09751">top-k sampling</a> to control the generation. And what‚Äôs more, there is no guarantee üò•.</li> <li><strong>Computational Constraints</strong> Sequential token-by-token generation incurs high computational costs, and the left-to-right modeling limits effectiveness in reversal reasoning tasks, the ‚Äú<a href="https://arxiv.org/abs/2309.12288">Reversal Curse</a>‚Äù.</li> </ul> </li> <li><strong>(Potential) Advantage of DLM</strong> <ul> <li><strong>NAR</strong> As sequence are generated holistically, you can fix and correct previous mistakes and refine the sequence as a whole.</li> <li><strong>Controllability</strong> Diffusion models are known to provide simple, training efficient style controlls using either classifier free guidance or classifier based guidance. Such controllability can be easily extended to DLM, where we can control the style of generation using the same techniques (<a href="https://arxiv.org/abs/2105.05233">Prafulla et al., 2021</a>, <a href="https://arxiv.org/abs/2103.00020">Radford et al., 2021</a>). We can also take this a step further to even more fine-grained controles in lengths, specific text editing, infillings because the model works on the whole sequence representation iteratively (<a href="https://arxiv.org/abs/2205.14217">Li et al, 2022</a>, <a href="https://arxiv.org/abs/2502.09992">Nie et al., 2025</a>,). This may also extends to tasks with stronger structural constraints (e.g., code, table).</li> <li><strong>Diversity</strong> Can produce more diverse outputs compared to beam search in AR. <em>You just need to sample different initial noise.</em></li> <li><strong>Speed Up</strong> Poential for faster generation as stops <em>could</em> be paralleilized, as tokens are generated all-together, instead of waiting for previous outputs iteratively.</li> </ul> </li> </ul> <h4 id="diffusion-model-recap"><strong>Diffusion Model Recap</strong></h4> <p>Diffusion models are very successful and widely adopted in computer vision tasks, such as image generation, super-resolution, and inpainting. The core idea of diffusion models is to learn a generative model by reversing a diffusion process that gradually adds noise to the data. Using the famous <a href="https://arxiv.org/abs/2006.11239">DDPM</a> as an example, given a data sample from a real data distribution \(\mathbf{x}_0 \sim \mathcal{D}(x)\), we use a <strong>forward process</strong> to gradually perturb the data with small amounts of Gaussian noise over \(T\) steps:</p> <p>\(\begin{equation} q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \end{equation}\) \(\begin{equation} q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}) \end{equation}\)</p> <p>where \(\beta_t \in (0, 1)\) is a variance schedule that controls the amount of noise added at each step. As \(T \to \infty\), \(\mathbf{x}_T\) approaches a sample from a standard Gaussian distribution:</p> \[\begin{equation} \lim_{T \to \infty} \mathbf{x}_T \approx \mathcal{N}(0, \mathbf{I}) \end{equation}\] <p>The <strong>reverse process</strong> then learns to gradually denoise the data, starting from pure noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\) and working backwards, where \(\mu_\theta\) and \(\Sigma_\theta\) are learned by a fancy neural network model. Again, if you are not comfortable with these concepts, please refer to <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian‚Äôs amazing blog</a>.</p> \[\begin{equation} p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \end{equation}\] \[\begin{equation} p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)) \end{equation}\] <div class="row mt-2"> <div class="col-sm-8 col-md-6 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/DDPM-480.webp 480w,/assets/img/diffusionlm_blog/DDPM-800.webp 800w,/assets/img/diffusionlm_blog/DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/DDPM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a> with a few additional annotations) </div> <h4 id="whats-the-fundamental-challenge"><strong>What‚Äôs the fundamental challenge?</strong></h4> <p>Well, if the Diffusion Model is well-established and it has all these exciting perks, why this is not as trending as in the field of computer vision? Good spot, now it comes to the fundamental challenge, the discrepancy between traditional <strong>continuous</strong> diffusion models, which achieved great success in image generation (such as with Denoising Diffusion Probabilistic Models - <a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a>), and the domain of <strong>discrete text</strong>.</p> <p>Think about an image. It‚Äôs made of pixels, and each pixel has colour values (like RGB) that are essentially numbers on a continuous scale. Adding ‚Äúnoise‚Äù is intuitive: you can slightly perturb these numerical values, typically by adding small random numbers drawn from a Gaussian (bell curve) distribution. Gradually adding more noise smoothly transitions the image to random static. The reverse process involves training a model to predict and subtract that small amount of noise at each step, gradually recovering the image.</p> <p>Now, consider text. Language is composed of discrete units ‚Äì words or tokens selected from a finite vocabulary (e.g., ‚Äúcat‚Äù, ‚Äúdog‚Äù, ‚Äúruns‚Äù). You can‚Äôt simply add ‚Äú0.1 Gaussian noise‚Äù to the word ‚Äúcat‚Äù and get something meaningful that‚Äôs slightly different but still related in a smooth, continuous way. Applying the original continuous noise formulation directly just doesn‚Äôt work.</p> <p>This <strong>discrete nature</strong> of text is the core hurdle that requires specific adaptations. How do you define a ‚Äúforward process‚Äù that gradually corrupts text into noise, and critically, a ‚Äúreverse process‚Äù that a model can learn to invert step-by-step?</p> <p>Researchers have developed clever workarounds to bridge this gap:</p> <ol> <li><strong>Operating on Continuous Variables:</strong> One approach is to work not with the tokens themselves, but with their continuous variables. Traditional langauge models gives well-constructed word embedding and hidden outputs representations. We could leverage these representations to define a continuous forward process, where the model learns to predict the noise added to these continuous representations at each step. This is similar to how diffusion models operate in the image domain, where the forward process is defined in the latent space of a VAE or similar architecture. <ul> <li><strong>Word Embedding (Token Level)</strong> Noise <em>can</em> be added to these word embedding vectors, a technique used in models like <a href="https://arxiv.org/abs/2205.14217">Diffusion-LM</a>, and the pre-trained DLM <a href="https://arxiv.org/abs/2212.11685">GENIE</a>. However, mapping potentially noisy embeddings back to specific discrete tokens at each step introduces its own complexities.</li> <li><strong>Higher Level Latent Representations</strong> Works like <a href="https://arxiv.org/abs/2306.02531">PLANNER</a> and <a href="https://arxiv.org/pdf/2212.09462">LD4LG</a> are operating on a higher level latent representations of paragraphs of texts. However, the latent representations are fragile when perturbed by noise, resulting in abrupt semantic changes in reverse diffusion process. <strong>My own paper</strong> <a href="https://arxiv.org/abs/2412.11333">SLD</a> attempts to mitigate this issue <em>cleverly</em> by text-segementation and improved representation learning techniques. Meta‚Äôs <a href="https://arxiv.org/pdf/2412.08821">Large Concept Model</a> is probably the <em>ultimate form</em> of pre-trained DLM following this path.</li> </ul> </li> <li><strong>Discrete Diffusion over Tokens:</strong> Now a few <em>bold geniuses</em> are thinking, ‚ÄúHey, if tokens are discrete, why not make the diffusion process discrete as well?‚Äù. So we have discrete diffusion over categorical supports. <a href="https://arxiv.org/pdf/2102.05379">Eminel et al, 2021</a> introduces extensions of diffusion for categorical data such as language. We model each token as a probability mass vector distributing over \(p \in \mathbb{R}^{V}\), where \(V\) is the size of the vocabulary and use transition matrices \(\mathbf{Q}\) to model transitions between denoising timestops, for example \(q(\mathbf{x}_t |\mathbf{x}_{t-1}) = Categorical(\mathbf{x}_t; \mathbf{p}=\mathbf{x}_{t-1}\mathbf{Q}_t)\). Models like <a href="https://arxiv.org/abs/2107.03006">D3PM</a>, and <a href="https://arxiv.org/pdf/2310.16834">SEDD</a> (<em>ICML 2024 Best Paper</em>) follows this path. More commonly, diffusion models for text define analogous discrete ‚Äúnoising‚Äù or ‚Äúcorruption‚Äù processes, as explored in papers like . Instead of adding mathematical noise, the forward process might involve: <ul> <li><strong>Masking:</strong> Randomly replacing tokens with a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token, with the number of masked tokens increasing over diffusion steps. The recent trending work of <a href="https://arxiv.org/pdf/2502.09992">LLaDA</a>, is the pre-trained DLM (8B parameters, largest of all kinds) following this path.</li> <li><strong>Token Substitution:</strong> Randomly replacing tokens with other tokens from the vocabulary.</li> <li><strong>Hybrids:</strong> Combining these or other discrete corruption methods.</li> </ul> </li> <li>üî•üñºÔ∏èüëä <strong>The Maverick‚ÄìText in Image:</strong> <em>‚ÄúDiscrete text? What text? It‚Äôs an image!‚Äù</em> I personally wish you to checkout this <a href="https://arxiv.org/pdf/2304.12519">GlyphDiffusion</a>. Instead of dealing with the discrepancy, they bypassed by rendering the target text as a glyph image containing visual language content ü§£.</li> </ol> <p>The reverse process then becomes about learning to <strong>undo</strong> this specific type of discrete corruption. For instance, the model learns to predict the original tokens at the <code class="language-plaintext highlighter-rouge">[MASK]</code> positions or identify and correct a randomly sampled variable into meaningful tokens , iteratively refining the sequence from a highly corrupted state back to coherent text. So, while the core <em>idea</em> of diffusion (iterative refinement from noise) remains, the <em>mechanisms</em> for the forward (corruption) and reverse (denoising) processes have to be specifically adapted for the discrete world of language.</p> <p>Now I will select the representative work of each of these methods to further explain the concept of DLM. <span style="color:blue"> For each of the paradigm, I will introduce papers explaining the mechanism and direct you to an off-the-shelf pre-trained model for you. </span></p> <h4 id="embedding-level-diffusion--where-it-begins"><strong>Embedding-Level diffusion ‚Äì where it begins</strong></h4> <h5 id="1token-level-embeddings"><strong>1.Token-level Embeddings</strong></h5> <p><em>As far as I know</em>, <a href="https://arxiv.org/abs/2205.14217">Diffusion-LM</a> is probably the first, influencial work that starts the era of DLM. Now suppose we have a sequence of words \(\mathbf{w} = \{w_1, w_2, \ldots, w_n\}\), an embedding fucntion is defined to map each word to a word vector \(Emb(w_i) \in \mathbb{R}^{d}\). So the entire sequence weill be encoded into \(\mathbf{x}_0 = Emb(\mathbf{w}) \in \mathbb{R}^{d}\). So yeah! There we go, we have a <strong>continuous</strong> space where we can run the conventional diffusion models. We use the typical simplified KL-divergence term in evidence lower bound (which I will not reiterate here) to derive a loss function.</p> \[\begin{equation} \mathcal{L}_{simple}(\mathbf{x}_0) = \Sigma_{t=1}^T \underset{q(\mathbf{x}_t | \mathbf{x}_0)}{\mathbb{E}} ||\mu(\mathbf{x}_t, t) - \hat{\mu}(\mathbf{x}_t, \mathbf{x}_0)||^2 \end{equation}\] <p>But don‚Äôt forget, we need to convert the embedding back to discrete tokens, you will say this is easy, let‚Äôs just have another function transform them back to tokens. Indeed, that‚Äôs how it‚Äôs done. In Li‚Äôs implementation, they model theses steps into the diffusion process as an extra timestep. As shown in the figure below, the forward process consists of an additional Markov transition to obtain the embeddings parametrized by \(q_{\phi}(\mathbf{x}_0 | \mathbf{w}) = \mathcal{N}(Emb(\mathbf{w}); \sigma_{0} I)\) . Then in the reverse process you will have an additional trainble <strong>rounding</strong> step, parameterized by \(p_{\theta}(\mathbf{w} | \mathbf{x}_0) = \prod_{i=1}^n p_{\theta}(w_i | x_i)\) , where \(p_{\theta}(w_i | x_i)\) is a simple softmax distribution.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/diffusion-lm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. A graphical model representing the forward and reverse diffusion processes for Diffusion-LM. (Image source: <a href="https://arxiv.org/abs/2205.14217">Li et al. 2022</a>) </div> <p>Then we can adjust the loss function accordingly. For end-to-end training, we will have the final loss function as below. During the reverse process, you run the inferece by sampling a random embedding sequence containing \(n\) token embeddings (fixed, the same as in the training process) and gradually remove the noise. You are always predicting \(n\) embeddings together, as the diffusion model requires a fixed shape I/O (A bit wasteful when the sequence is underfull right? We will talk about it later.).</p> \[\begin{equation} \mathcal{L}_{simple}^{e2e}(\mathbf{w}) = \underset{q_{\phi}(\mathbf{x}_{0:T} | \mathbf{w})}{\mathbb{E}} \left[\underbrace{\mathcal{L}_{simple}(\mathbf{x}_0)}_{\text{diffusion Loss}} + ||Emb(\mathbf{w}) - \overbrace{\mu_{\theta}(\mathbf{x}_1, 1)}^{\text{predicted }\mathbf{x}_0}||^2 - \underbrace{\log p_{\theta}(\mathbf{w} | \mathbf{x}_0)}_{\text{rounding}} \right] \end{equation}\] <p>So, everything seems extremely straightforward right? Or does it? Unfortunately, no üòÖ. The conversion between continous embedding space and discrete tokens is non-trivial, harder than you think. <span style="color:red">This rounding is a key challenge in token embedding-level diffusion models. The discretization step can lead to errors that accumulate across the diffusion process, as the embedding space is not uniformly populated with valid tokens.</span> <em>Well isn‚Äôt this the notorious data sparsity revisited.</em></p> <p>In the paper, there is a major chapter talking about the techniques of how they managed to reduce the rounding error to obtain admissible outputs. For example using reparameterisation trick to make sure every term in the loss models \(\mathbf{x}_0\) explicitly. They also introduce a <strong>clamping trick</strong> that is maping the predicted vector \(\mathbf{x}_t\) to its nearest word embedding sequence in every reverse diffusion sampling step. Still, a lot of work needs to be done in the future for the sake of generation quality.</p> <p>Back to the model, with the diffusion pipeline, you can do the fancy conditioning and controlled generation during your inference now. For example, you could have a separate neural network classifier and a class condition \(\mathbf{c}\). During the backward process, you obtain the \(\mathbf{x}_{t-1}\) with respect to the posterior probability using the gradient update below.</p> \[\begin{equation} \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{c}) = \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}) + \underbrace{\nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{c} | \mathbf{x}_{t-1})}_{\text{classifier guidance}} \end{equation}\] <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-classifier-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/diffusion-lm-classifier.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. For controllable generation, we iteratively perform gradient updates on these continuous latents to optimize for fluency (parametrized by Diffusion-LM) and satisfy control requirements (parametrized by a classifier). (Image source: <a href="https://arxiv.org/abs/2205.14217">Li et al. 2022</a>) </div> <p>The paper provides many experiments for controlled generation (e.g., semantics, length, POS and etc.). I want to mention about <strong>infilling</strong> specifically. That is during the inference process, instead of denoising all the embeddings, some context embeddings are given and fixed (e.g., \(\mathbf{x}_t =\)<code class="language-plaintext highlighter-rouge">[w_1] [noise] [w_2]</code>), the diffusion model will mask the gradient of other tokens, only generated the noised token in the middle. However, the other embeddings are used together as conditions naturally during the reverse sampling, as a <strong>classifier-free guidance</strong>. And you, my clever reader, immediately understands how traditional sequence-to-sequence task can be modelled by giving the input as left contexts only.</p> <p><a href="https://arxiv.org/pdf/2212.11685">GENIE</a> is the first pre-trained DLM (<em>again, as I know</em>) following this token embedding-level diffusion. The model uses <strong>continuous paragraph denoise</strong> objective for pre-training. The object first applied a hybrid noising techniques to the original text, including token masking (e.g., <code class="language-plaintext highlighter-rouge">[w_1] [MASK] [w_2]</code>) and forward diffusion process sampling, then ask the model to recover the clean original text.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GENIE-480.webp 480w,/assets/img/diffusionlm_blog/GENIE-800.webp 800w,/assets/img/diffusionlm_blog/GENIE-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/GENIE.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. The framework of GENIE. Source sequence is encoded as the condition of the transformer DLM through cross attention. DLM restores the randomly initial Gaussian noise to the output text through the iterative denoising and grounding process (Image source: <a href="https://arxiv.org/pdf/2212.11685">Lin et al., 2022</a>) </div> <p>Notably, GENIE doesn‚Äôt use infilling as the default sequence-to-sequence generation method. Instead, it is using a more Encoder-Decoder approach (yes, like BART or T5), which is another analogues of classifier-free guidance. Similar rounding techniques is applied, they are using an effective KNN algorithm to retreive closets word embedding of each token during reverse sampling and apply rounding to the closest learnt word embedding.</p> <h5 id="2-higher-level-embeddings"><strong>2. Higher-level Embeddings</strong></h5> <p>To tackle this <strong>rounding error</strong> of translating predicted word-embeddings to discrete tokens, attempts have been made to bring diffusion to high-level semantic latent space (e.g., sentences, paragraphs). The diffusion will predict some latent representation of a piece of text, and a separate autoregressive decoder is used for decoding the representation back to text. We use the work of Lovelace et al., Latent Diffusion For Language Generation (<a href="https://arxiv.org/abs/2212.09462">LD4LG</a>) as an example to explain the concept.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LD4LG-480.webp 480w,/assets/img/diffusionlm_blog/LD4LG-800.webp 800w,/assets/img/diffusionlm_blog/LD4LG-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/LD4LG.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Overview of Latent Diffusion For Language Generation (Image source: <a href="https://arxiv.org/abs/2212.09462">Lovelace et al., 2023</a>) </div> <p>Firstly, an encoder model (e.g. BART or T5 Encoder) \(Enc()\) to convert the piece of text \(\mathbf{w} = {w_1, \ldots, w_n}\) into hidden states, \(Enc(\mathbf{w}) \in \mathbb{R}^{n \times h_{lm}}\), where \(h_{lm}\) is the hidden state dimension of your encoder (usually 768 or 1024‚Ä¶). Notice that, \(n\) here represents the variable token length of the text, and the diffusion requires a fixed shape representation. In the token-level embedding diffusion, this is a constraint for the context window. For higher-level embeddings, an additional pooling/compressing unit \(f()\) is used to project the hidden states to a fixed-length latent representation \(\mathbf{z} = f(Enc(\mathbf{w})), \mathbf{z} \in \mathbb{R}^{k \times h_{rep}}\), where \(k\) and \(h_{rep}\) are the dimension of the latent representation (hyperparameters). Typically you want \(k&lt;n\) and \(h_{rep} &lt;&lt; h_{lm}\), as you don‚Äôt want a very sparse latent representation, a more compact latent space helps diffusion model to learn the distribution more easily.</p> <p>Then the diffusion model \(R(\cdot |\theta)\) , which is usually a diffusion transformer (<a href="https://arxiv.org/abs/2212.09748">DiT</a>) deployed to learn how to recover \(\mathbf{z}_0\) from its noised version over the forward diffusion process as usual.</p> \[\begin{equation} \mathcal{L}(\theta_{R}) = \sum_{t=1}^T \underset{q(\mathbf{z}_{t} | \mathbf{z}_0)} {\mathbb{E}} \left\| \hat{\mathbf{z}}_{t} - \mathbf{z}_t \right\|^2_2 \end{equation}\] <p>For inference, you sample a latent representation from gaussian noise, and do the reverse process as usual \(\hat{\mathbf{z}}_{t-1} = R(\hat{\mathbf{z}}_t, t); \theta_{R}\). Conditioning can be added to forward and backward process similarly as well. Then we employ a reconstruction unit \(g()\), projecting the latent state back to the dimension of the backbone language model hidden state, using the decoder to generate text \(Dec(g(\mathbf{z}_0))\). In this case, as we are always doing diffusion over the continuous latent space of high level semantics, we bypass the phase of rounding.</p> <p><strong>BUT</strong>, again, is it that simple? Actually, no ‚òπÔ∏è. <span style="color:red"> It is non-trivial to find a good latent representation for text, not mentioning generate one from noisy examples. </span> For example, if I perturb a latent representation of sentence <code class="language-plaintext highlighter-rouge">It is sunny today</code> a bit, in this case, what are we doing here? Just as it‚Äôs name that high-level semantic diffusion, it should preturb it‚Äôs semantics, not surface forms (tokens) right? We want to make sure I add a small noise to the sentence, it should give me some thing similar in meaning like <code class="language-plaintext highlighter-rouge">Today is sunny</code> or <code class="language-plaintext highlighter-rouge">Today has sun</code> some thing like that, definitely not <code class="language-plaintext highlighter-rouge">It isn't sunny today</code> or <code class="language-plaintext highlighter-rouge">It is savvy tody</code> which is similar by appearance but not in meaning. But this is extremely hard to do, as a the meaning of a paragraph of text is extremely rich and diverse, how are we suppose to regularize that? This pose a great challenge for high-level semantic diffusion doing long-form generation.</p> <p>So back to square one, what‚Äôs the definition of a good latent representation for high-level semantics diffusion? In <a href="https://arxiv.org/pdf/2306.02531">Zhang et al., 2024</a> gives a formal definition of this, describing the desirata in three aspects.</p> <ul> <li> <p><strong>Low conversion error</strong> Give you a piece of text \(\mathbf{w}\), we can transform it into latent representations and convert it back, \(\tilde{\mathbf{w}} = Dec(g(f(Enc(\mathbf{w}))))\). The difference between \(\mathbf{w}\) and \(\tilde{\mathbf{w}}\) should be minimal, or none. However this is not very achievable when \(\mathbf{w}\) is long. The hyperparameter \(k, h_{rep}\) are fixed, when we are doing projections, we are losing information. The longer the sequence, the more information is lost, right?</p> </li> <li> <p><strong>Local Smoothness</strong> Consider if a person stutters or drop a few minor words while speaking, you can still recover what the person is trying to convey. That said, given a piece of text \(\mathbf{w}\), and its slightly variant version \(\mathbf{w'}\), their encoded latent representation should not differ to much, \(\mathbf{z}_{w} \approx \mathbf{z}_{w'}\).</p> </li> <li> <p><strong>Distributional Smoothness</strong> In the high level latent space, we want the meanings of paragraphs are distributed smoothly. That is consider you have a piece of text \(\mathbf{w}\) and it‚Äôs latent representation \(\mathbf{z}_{w}\), in the latent space it should be closer to similar meanings, like paraphrases of \(\mathbf{w}\). When we are preturbing the text with a small amount of text, the decoded representation shouldn‚Äôt differ to much by its meaning, as mentioned above. Vice versa, text with different meanings should be far away from each other in the latent space. However, when the sequence gets longer it‚Äôs hard to control this as it contains too much complex concepts and meanings. If we increase the size of the latent space, the diffusion model may face difficulty in learning a distribution, \(p(\mathbf{z})\) that is highly multimodal, or has density that are associated with a large Lipchitz constant (i.e., has abrupt changes).</p> </li> </ul> <p>So the fact is without proper regularization, the learned distribution may be susceptible to abrupt semantic changes due to small perturbations, increasing the difficulty of the task for the diffusion model and catastrophically corrupts the quality of decoded texts. So how do we fix this?</p> <p><a href="https://arxiv.org/pdf/2412.11333">Zhu et al., 2024</a> (<em>yeah that‚Äôs me ü§™</em>) provides Segment-Level Diffusion. Extending the concept <strong>patching</strong> from image generation, we patch texts into segments (e.g. sentences, dialogue utterances). This way we have effectively control the length of text and the meanings in the segment. We further regulate the latent representations by doing additional training for representation learning, using contrastive learning, and adversial noise preturbation, ensuring local and distributional smoothness. The diffusion model will now predict multiple latent representations with one-to-one correspondence to each segments. These representations will be independently decoded in parallel.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/SLD-480.webp 480w,/assets/img/diffusionlm_blog/SLD-800.webp 800w,/assets/img/diffusionlm_blog/SLD-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/SLD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Overview of the training pipeline of SLD. In the first stage, gold output is divided into segments. In the second stage, we use contrastive and adversarial learning to ensure latent representations are robust to drastic semantic changes. Finally, we train a diffusion model as an inherent semantic planner conditioned on given inputs. (Image source: <a href="https://arxiv.org/pdf/2412.11333">Zhu et al., 2024</a>) </div> <p>A contemporary work from Meta, <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/">Large Concept Model</a> uses a similar paradigm. They are doing diffusion over concepts, which is the segment in my case. Do check their work out! They provide the model pre-trained on much more data than I have! <em>I was submitting mine for 15th of Dec 2024 ARR, but they released their paper on the 12th. Well I‚Äôd be lying if I say I am not a bit frustrated. But hey! This also proves my idea, our idea, is very promising! Hope this is helpful to you!</em></p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LCM-480.webp 480w,/assets/img/diffusionlm_blog/LCM-800.webp 800w,/assets/img/diffusionlm_blog/LCM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/LCM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Left: visualization of reasoning in an embedding space of concepts (task of summarization). Right: fundamental architecture of an Large Concept Model (LCM). Note that the LCM is a diffusion model! (Image source: <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/"> LCM Team 2024</a>) </div> <h4 id="discrete-diffusion-over-tokens"><strong>Discrete Diffusion over Tokens</strong></h4> <p>In this section, we describe how diffusion over tokens is done using Masked Diffusion Model (MDM) by using examples of Sahoo et al., ‚Äòs <a href="https://arxiv.org/abs/2406.07524v2">Masked Diffusion Language Models</a> as a concrete sample. Please also check out <a href="https://arxiv.org/abs/2107.03006">D3PM</a>, and <a href="https://arxiv.org/pdf/2310.16834">SEDD</a> for more details, but they are too mathematically dense and terse for this introduction blog.</p> <p>As mentioned before, <a href="https://arxiv.org/abs/2107.03006">D3PM</a> introduces the Markov forward process as categorical distributions using multiplications of matrices \(\mathbf{Q}_{t}\) over \(T\) discrete timesteps. We have a series of multiplication \(\mathbf{Q}_{T-1}\cdot\mathbf{Q}_{T-1} \ldots \mathbf{Q}_{1} \mathbf{x}\) that converges to a stationary distribution.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/MDLM-480.webp 480w,/assets/img/diffusionlm_blog/MDLM-800.webp 800w,/assets/img/diffusionlm_blog/MDLM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/MDLM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> (Left) Masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling.(Bottom Right) Perplexity (PPL) on One Billion Words benchmark. (Image source: <a href="https://arxiv.org/abs/2406.07524v2"> Sahoo et al., 2024</a>) </div> <p>In their work, they have tokens represented as \(\mathbf{x} \in \mathcal{V}\), where \(\mathcal{V}, |\mathcal{V}| = K\) is a set of all the one-hot vectors of the vocabulary. They define the \(Cat(\cdot;\mathbf{\pi})\) as the categorical distribution over K token classes with probabilities given by \(\mathbf{\pi} \in \Delta ^ K\), where \(\Delta ^ k\) denotes the \(K\)-simplex. They also define a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token \(\mathbf{m} \in \mathcal{V}\).</p> <p>During the forward process, they interpolate the discrete diffusion by converting \(\mathbf{x}\) into increasingly noisy variables \(\mathbf{z}_t\). The marginal of \(\mathbf{z}_t\) conditioned on \(\mathbf{x}\) is given below, \(\alpha\) is still the term derived from noise schedules in standard diffusion process.</p> \[\begin{equation} q(\mathbf{z}_t | \mathbf{x}) = Cat(\mathbf{z}_t; \alpha_t \mathbf{x} + (1-\alpha_t)\mathbf{\pi}) \end{equation}\] <p>During masked diffusion, \(\pi = \mathbf{m}\), which means at each noising step, t, the input x transitions to a ‚Äúmasked‚Äù state \(\mathbf{m}\) with some probability. If an input transitions to \(\mathbf{m}\) at any time \(t\), it will remain as the masked token afterwards.</p> \[\begin{equation} q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x}) = \begin{cases} \text{Cat}(\mathbf{z}_s; \mathbf{z}_t), &amp; \mathbf{z}_t \neq \mathbf{m},\\ \text{Cat}\left( \mathbf{z}_s; \frac{(1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t)\mathbf{x}}{1 - \alpha_t} \right), &amp; \mathbf{z}_t = \mathbf{m}. \end{cases} \end{equation}\] <table> <tbody> <tr> <td>So, consequently, for the reverse diffusion process, we train a network to do $$p_{\theta} (\mathbf{z}_s</td> <td>\mathbf{z}_t)$$ to convert masked tokens back to concrete tokens.</td> </tr> </tbody> </table> \[\begin{equation} p_\theta(\mathbf{z}_s \mid \mathbf{z}_t) = q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x} = \mathbf{x}_\theta(\mathbf{z}_t, t)) = \begin{cases} \text{Cat}(\mathbf{z}_s; \mathbf{z}_t), &amp; \mathbf{z}_t \neq \mathbf{m}, \\ \text{Cat}\left(\mathbf{z}_s; \frac{(1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t)\mathbf{x}_\theta(\mathbf{z}_t, t)}{1 - \alpha_t} \right), &amp; \mathbf{z}_t = \mathbf{m}. \end{cases} \end{equation}\] <p>Great, right? In this case they have successfully extended the diffusion from continous into discrete domain. I‚Äôve omitted a few details which you can check out in detail in their <a href="https://s-sahoo.com/mdlm/">blog</a>, or their paper. However, in their project, note that their design is not flexible enough. <em>While the prototype naturally has some limitations typical of early-stage systems, it marks a significant advancement in the field.</em> For example, during decoding, the token stays the same after it is unmasked. This is not ideal, as in early stage of the reverse diffusion, the entire paragraph is noisy, and the few tokens decoded will are probably not optimal. But since they are fixed, your later decoding process is conditioned on these tokens causing <strong>error-propagation</strong> again. <a href="https://arxiv.org/abs/2302.05737">Lin et al.,</a> from <a href="https://ikekonglp.github.io/">Lingpeng Kong‚Äôs Group</a> (<em>I like their work a lot</em>) therefore uses a <strong>routing</strong> technique to mitigate this issue. Basically they model that a decoded token can be remasked into <code class="language-plaintext highlighter-rouge">[MASK]</code> or noised token during the decoding stage if model‚Äôs confidence on that token is low.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LLaDA.svg" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/LLaDA.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> (a) Pre-training. LLaDA is trained on text with random masks applied independently to all tokens at the same ratio t ‚àº U[0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion process from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies. (Image source: <a href="https://arxiv.org/abs/2502.09992"> Nie et al., 2025</a>) </div> <p>Finally, <a href="https://arxiv.org/abs/2502.09992">LLaDA 8B</a> combines both techniques above, MDLM and routing, demonstrating the potential of MDM as a new paradigm of pre-trained language models with on-par and even superior generation quality than AR LLMs in the same size. Amazing isn‚Äôt it? Try <a href="https://huggingface.co/spaces/multimodalart/LLaDA">it</a> out yourself!</p> <h4 id="text-in-image-diffusion-brain-teaser"><strong>Text-in-Image Diffusion? (Brain-teaser)</strong></h4> <p>Think CV! Nowadays, we are borrowing concepts from each other for a long time. Like <a href="https://arxiv.org/abs/2404.02905">VAR</a> styled auto-regressive image generation. So maybe we can go a step further since we are already using diffusion, the most trending paradigm in CV. But I will skip great CV works like <a href="https://github.com/deep-floyd/IF">DeepFloyd IF</a> for conciseness. As a brain-teaser, I will just introduce this <a href="https://arxiv.org/pdf/2304.12519v2">GlyphDiffusion</a>. Their key idea is to render the target text as a glyph image containing visual language content. The conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. <em>Let‚Äôs stop worrying whether this is scalable or not, at least it‚Äôs fun.</em></p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GlyphDiffusion-480.webp 480w,/assets/img/diffusionlm_blog/GlyphDiffusion-800.webp 800w,/assets/img/diffusionlm_blog/GlyphDiffusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/GlyphDiffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GlyphDiffusion generate patches of image containing visual language contents. (Image source: <a href="https://arxiv.org/abs/2304.12519v2"> Li et al., 2023</a>) </div> <h4 id="challenges-and-oppurtunities"><strong>Challenges and Oppurtunities</strong></h4> <p>You might wonder, ‚ÄúHey after reading through this blog, seems like the concept of DLM is well-established as we have already have all the pre-trained models already. What stops us from using it?‚Äù Well, the major challenge is <strong>speed</strong>, the efficiency and computational cost during inference sampling. I know that we claimed NAR methods could be potentially faster than traditional AR methods, but we haven‚Äôt achieve that so far. AR methods is benefiting from acceleration techniques like KV caching, which is not applicable for NAR generation. Now naive DLM implementations is about 50 times slower than AR methods. <strong>But!</strong> Don‚Äôt get frustrated, we are gradually adapting the techniques of acceleration to this field, (e.g., <a href="https://openai.com/index/simplifying-stabilizing-and-scaling-continuous-time-consistency-models/">Consistency Models</a>, <a href="https://arxiv.org/abs/2305.04465">Adaptive Decay Sampling</a>, etc.). Yeah, another work (<a href="https://arxiv.org/abs/2402.07754">Ye et al., 2024 Diffusion of Thoughts</a>), Diffu from Kong‚Äôs group has succesfully used ODE solver to speed up the decoding process. I believe we will catch up soon. And I think some institute has already done it, check out Inception Labs‚Äô <a href="https://chat.inceptionlabs.ai/">Mercury Coder</a>, the first, fast, commercial DLM!</p> <p>There are lots of potential of DLMs yet to be explored. For example, since we are generating everything all together, how does this changes searching/reasoning? Will this give us better CoT results, avoiding bad trajectories if the model can look ahead? Checkout <a href="https://arxiv.org/abs/2402.07754">Diffusion-of-Thought</a> and <a href="https://hkunlp.github.io/blog/2025/dream/">Dream 7B Diffusion Reasoning Model</a>! Since we can use diffusion to do generation with fine-grained controls such as infilling easily, we can generate better data with complex grammar/format constraints (e.g., tables, code) with guarantees. Check out <a href="https://arxiv.org/pdf/2410.20626">TabDiff</a> and <a href="https://arxiv.org/abs/2407.02549">Mario et al., 2024</a>‚Äôs work. The random sampling of diffusion can also help us augment data easily, which might be useful for expensive/rare data, such as low-resource langauge. <a href="https://aclanthology.org/2024.emnlp-main.109.pdf">Chen et al, 2024</a> used this for low-resource sentiment classification. We can even finally start unifying all the modalities, generating image and tokens all at once, like Meta‚Äôs <a href="https://ai.meta.com/research/publications/transfusion-predict-the-next-token-and-diffuse-images-with-one-multi-modal-model/">Transfusion</a>! <em>But I think we have to call it FusionTrans, as they are using AR for image, we are using Diffusion for text.</em></p> <p>Yeah, these people are continuously updating the list of existing <a href="https://github.com/bansky-cl/diffusion-nlp-paper-arxiv">DLM papers</a>.</p> <h5 id="epilogue"><strong>Epilogue</strong></h5> <p>I will end this introductory blog here without a conclusion, as the Era of DLM has just begun üöÄ ! I hope this blog helps you in understanding DLMs, and make you better prepared or even sparkled some ideas if you want to do any DLM related research. <em>Oh Gosh, I hope I didn‚Äôt write too much dad jokes.</em> If you guys like it, I will probably write more blogs to describe some aspects in depth in the future. If you want to do me a favour and find this blog helpful, you can cite my <a href="https://arxiv.org/abs/2412.11333">Segment-level Diffusion</a> paper in your paper, as I‚Äôve included most of the blogs content into my related work section. Leave a comment if you have any suggestions for the blog.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhu2024segmentleveldiffusionframeworkcontrollable</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models}</span><span class="p">,</span> 
  <span class="na">author</span><span class="p">=</span><span class="s">{Xiaochen Zhu and Georgi Karadzhov and Chenxi Whitehouse and Andreas Vlachos}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span><span class="p">=</span><span class="s">{2412.11333}</span><span class="p">,</span>
  <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.11333}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> <p>Until next time‚Ä¶</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A gentle, in-depth introduction of existing diffusion language models.]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://spacehunterinf.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://spacehunterinf.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://spacehunterinf.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/tabs</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="7744a50e-e1e5-441b-931a-f007cc928673" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="7744a50e-e1e5-441b-931a-f007cc928673" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="9719dc2d-a03a-42e0-a08a-343d06afbba3" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="9719dc2d-a03a-42e0-a08a-343d06afbba3" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="eae492c6-178c-47b4-9157-56bbc8b264cd" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="eae492c6-178c-47b4-9157-56bbc8b264cd" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://spacehunterinf.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/typograms</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://spacehunterinf.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://spacehunterinf.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://spacehunterinf.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://spacehunterinf.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">a post with vega lite</title><link href="https://spacehunterinf.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="a post with vega lite"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry></feed>