<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://spacehunterinf.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://spacehunterinf.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-29T21:44:01+00:00</updated><id>https://spacehunterinf.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Xiaochen Zhu.. </subtitle><entry><title type="html">What are Diffusion Language Models?</title><link href="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/" rel="alternate" type="text/html" title="What are Diffusion Language Models?"/><published>2025-04-14T11:59:00+00:00</published><updated>2025-04-14T11:59:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2025/diffusion-language-models</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/"><![CDATA[<h4 id="preface"><strong>Preface</strong></h4> <p><a href="https://www.youtube.com/watch?v=X0Jti9F-oQA">Dear Reader</a>,<br/> I‚Äôve been wanting to write this blog for a long time. Diffusion models for language generation are super exciting ‚Äî an emerging field that‚Äôs getting increasing attention. But up until now, there hasn‚Äôt really been a comprehensive guide or intro for folks in the NLP/ML community who want to dive into this area and maybe even start doing research. So here we are! In this blog, we‚Äôll walk through the history of diffusion language models, different paradigms for building them, some future research directions and applications ‚Äî <em>plus a few of my own (possibly biased) personal opinions, italicized for your reading pleasure.</em> I‚Äôll also keep updating this blog over time, and hey, who knows ‚Äî maybe it‚Äôll grow into a full survey paper one day.</p> <p>This blog is mainly for people who already know a decent bit about Diffusion Models and good old autoregressive LLMs. If that‚Äôs not you yet, no worries ‚Äî here are some resources I found super helpful:</p> <ul> <li><strong>For Diffusion Models</strong>: <ul> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a> by Lilian Weng (<em>strongly recommended!</em>)</li> <li><a href="https://arxiv.org/abs/2208.11970">Understanding Diffusion Models: A Unified Perspective</a> by Calvin Luo</li> </ul> </li> <li><strong>For Autoregressive LLMs</strong>: <ul> <li><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&amp;list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&amp;index=8">Stanford CS224N</a> (<em>GOAT course, trust me</em>)</li> </ul> </li> </ul> <hr/> <h4 id="whats-a-diffusion-language-model-dlm"><strong>What‚Äôs a Diffusion Language Model (DLM)?</strong></h4> <p>Quick recap: all the trendy autoregressive language models (AR-LMs) these days ‚Äî GPT-2, Llama, Gemini, ChatGPT, Claude, you name it ‚Äî use the Transformer backbone for <strong>autoregressive</strong> (AR) decoding. That means they predict one token at a time, left-to-right.</p> <p>Diffusion Language Models (DLMs), on the other hand, work differently. Instead of going token by token, they <strong>iteratively refine</strong> and predict the <em>whole</em> sequence from a noisy starting point ‚Äî following a <strong>non-autoregressive</strong> (NAR) decoding process.</p> <p>Here‚Äôs a (<em>very simplified</em>) way to picture the difference between the two paradigms, shown in the figure below:</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion_vs_ar-480.webp 480w,/assets/img/diffusionlm_blog/diffusion_vs_ar-800.webp 800w,/assets/img/diffusionlm_blog/diffusion_vs_ar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/diffusion_vs_ar.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. AR-LM predicts the sequence token-by-token. Output tokens are used as input for the next token-prediction in left-to-right manner (top). DLM iteratively refines the entire output sequence from a noisy sample (bottom). </div> <p>Putting it in mathematical terms: Suppose we want to predict a sequence \(\mathbf{x} = \{x_1, x_2, \ldots, x_N\}.\) An AR-LM (autoregressive language model) with parameters \(\theta\) models the following distribution:</p> \[\begin{equation} \label{eq:AR-LM} P(\mathbf{x}; \theta) = \prod_{n=1}^{N} P(x_n \mid \mathbf{x}_{&lt;n}; \theta) \end{equation}\] <p>In contrast, DLMs take a <em>holistic</em> view of the entire sequence. They model a different kind of distribution ‚Äî one that evolves over time \(t\) in a <strong>reverse diffusion process</strong> (<em>don‚Äôt worry, we‚Äôll get into the details very soon</em>). Here, a larger \(t\) corresponds to a noisier version of the sequence ‚Äî something closer to random Gaussian noise.<br/> You can think of it like we start with a super messy paragraph and then <em>iteratively <a href="https://www.youtube.com/watch?v=AppsjTInqiw">CLEAN</a> it up</em> until it becomes the polished passage we actually want.</p> \[\begin{equation} \label{eq:DLM} \mathbf{x}_{t-1} \sim p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, t) \end{equation}\] <h4 id="why-do-we-need-dlms-or-nar-whats-wrong-with-ar"><strong>Why do we need DLMs (or NAR)? What‚Äôs wrong with AR?</strong></h4> <p>Autoregressive models are insanely successful these days ‚Äî so why bother with another paradigm? Why is DLM a field worth looking into? Well, here are some arguments for you to consider:</p> <hr/> <ul> <li><strong>Inherent Disadvantages of AR-LMs</strong> <ul> <li> <p><strong>Error Propagation</strong>:<br/> In autoregressive models, if you make a mistake when predicting the current token, tough luck ‚Äî you can‚Äôt go back and fix it. Future predictions are based on that flawed token, causing errors to propagate and accumulate over time. This painful phenomenon is known as <a href="https://aclanthology.org/D18-1396/">error propagation</a>.</p> </li> <li> <p><strong>Indirect Generation Control</strong>:<br/> Controlling AR generation is tricky. Most methods rely on heavy training or hacks during decoding ‚Äî and honestly, they‚Äôre pretty inconvenient. For example, if you want to generate a passage of a certain length, you either have to train a separate length predictor or do fancy prompting. Other controls my rely on heuristics like <a href="https://arxiv.org/abs/1904.09751">top-k sampling</a>. And even then‚Ä¶ there‚Äôs no guarantee it‚Äôll work üò•.</p> </li> <li> <p><strong>Computational Constraints</strong>:<br/> Token-by-token generation is slow and computationally expensive. Plus, the strict left-to-right setup limits tasks that require reverse reasoning ‚Äî a problem known as the ‚Äú<a href="https://arxiv.org/abs/2309.12288">Reversal Curse</a>‚Äù.</p> </li> </ul> </li> </ul> <hr/> <ul> <li><strong>(Potential) Advantages of DLMs</strong> <ul> <li> <p><strong>Non-Autoregressive (NAR) Generation</strong>:<br/> Since sequences are generated holistically, the model can fix earlier mistakes as it refines the output ‚Äî no more getting stuck with bad early guesses.</p> </li> <li> <p><strong>Controllability</strong>:<br/> Diffusion models are naturally good at controllable generation! Using tricks like classifier-free guidance or classifier-based guidance (<a href="https://arxiv.org/abs/2105.05233">Prafulla et al., 2021</a>, <a href="https://arxiv.org/abs/2103.00020">Radford et al., 2021</a>), we can easily steer the output style. In DLMs, this can extend even further ‚Äî allowing fine-grained control over length, specific text edits, infillings, and structural constraints like code and tables (<a href="https://arxiv.org/abs/2205.14217">Li et al., 2022</a>, <a href="https://arxiv.org/abs/2502.09992">Nie et al., 2025</a>).</p> </li> <li> <p><strong>Diversity</strong>:<br/> Want different outputs? Just sample different initial noise ‚Äî no fancy beam search or sampling needed üé≤.</p> </li> <li> <p><strong>(<em>Potential</em>) Speed Up</strong>:<br/> Since generation doesn‚Äôt have to happen strictly token-by-token, there‚Äôs potential for faster, more parallelized decoding.</p> </li> </ul> </li> </ul> <hr/> <h4 id="diffusion-model-recap"><strong>Diffusion Model Recap</strong></h4> <p>Diffusion models are very successful and widely adopted in computer vision tasks, such as image generation, super-resolution, and inpainting. The core idea of diffusion models is to learn a generative model by reversing a diffusion process that gradually adds noise to the data. Using the famous <a href="https://arxiv.org/abs/2006.11239">DDPM</a> as an example, given a data sample from a real data distribution \(\mathbf{x}_0 \sim \mathcal{D}(x)\), we use a <strong>forward process</strong> to gradually perturb the data with small amounts of Gaussian noise over \(T\) steps:</p> <p>\(\begin{equation} q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \end{equation}\) \(\begin{equation} q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}) \end{equation}\)</p> <p>where \(\beta_t \in (0, 1)\) is a variance schedule that controls the amount of noise added at each step. As \(T \to \infty\), \(\mathbf{x}_T\) approaches a sample from a standard Gaussian distribution:</p> \[\begin{equation} \lim_{T \to \infty} \mathbf{x}_T \approx \mathcal{N}(0, \mathbf{I}) \end{equation}\] <p>The <strong>reverse process</strong> then learns to gradually denoise the data, starting from pure noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\) and working backwards, where \(\mu_\theta\) and \(\Sigma_\theta\) are learned by a fancy neural network model. Again, if you are not comfortable with these concepts, please refer to <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian‚Äôs amazing blog</a>.</p> \[\begin{equation} p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \end{equation}\] \[\begin{equation} p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)) \end{equation}\] <div class="row mt-2"> <div class="col-sm-8 col-md-6 mt-2 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/DDPM-480.webp 480w,/assets/img/diffusionlm_blog/DDPM-800.webp 800w,/assets/img/diffusionlm_blog/DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/DDPM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a> with a few additional annotations) </div> <h4 id="whats-the-fundamental-challenge"><strong>What‚Äôs the Fundamental Challenge?</strong></h4> <p>Well, if Diffusion Models are so well-established and come with all these exciting perks, why aren‚Äôt they as trending in NLP as they are in computer vision? üëÄ</p> <p>Good question ‚Äî and now we get to the fundamental challenge: there‚Äôs a big discrepancy between traditional <strong>continuous</strong> diffusion models (which crushed it in image generation ‚Äî see Denoising Diffusion Probabilistic Models, <a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a>) and the world of <strong>discrete text</strong>.</p> <p>Think about an <strong>image</strong>: it‚Äôs made of pixels, and each pixel has color values (like RGB) ‚Äî basically numbers on a continuous scale. Adding ‚Äúnoise‚Äù is super intuitive: you just perturb those numbers a little, typically by adding random Gaussian noise. Gradually adding more and more noise smoothly transitions the image into random static. And the reverse process? Just train a model to predict and subtract that noise step-by-step, and voil√† ‚Äî the original image comes back.</p> <p>Now, consider <strong>text</strong>: language is made of <strong>discrete</strong> units ‚Äî words or tokens picked from a finite vocabulary (‚Äúcat‚Äù, ‚Äúdog‚Äù, ‚Äúruns‚Äù, etc.). You can‚Äôt just add ‚Äú0.1 Gaussian noise‚Äù to the word ‚Äúcat‚Äù and expect to get something slightly fuzzier but still meaningful. Applying the same continuous noise idea directly <em>just doesn‚Äôt work</em>.</p> <p>This <strong>discrete nature</strong> of text is the core hurdle. The big question is:</p> <blockquote> <p>How do you define a ‚Äúforward process‚Äù that gradually corrupts text into noise ‚Äî and, critically, a ‚Äúreverse process‚Äù that a model can learn to invert, step-by-step?</p> </blockquote> <p>Researchers have developed some clever workarounds to bridge this gap:</p> <hr/> <p><strong>Operating on Continuous Variables</strong></p> <p>One approach is to not work with the tokens themselves, but with their continuous variables. Traditional language models already produce well-constructed word embeddings and hidden layer outputs. We can leverage these representations to define a continuous forward process, where the model learns to predict the noise added to these continuous vectors at each step. This is similar to how diffusion models operate in the image domain ‚Äî often working in the latent space of a VAE or similar architecture.</p> <ul> <li> <p><strong>Word Embedding (Token Level)</strong>:<br/> Noise <em>can</em> be added to word embedding vectors ‚Äî a technique used in models like <a href="https://arxiv.org/abs/2205.14217">Diffusion-LM</a> and the pre-trained DLM <a href="https://arxiv.org/abs/2212.11685">GENIE</a>. However, mapping potentially noisy embeddings back to specific discrete tokens at each step introduces its own complexities.</p> </li> <li> <p><strong>Higher-Level Latent Representations</strong>:<br/> Works like <a href="https://arxiv.org/abs/2306.02531">PLANNER</a> and <a href="https://arxiv.org/pdf/2212.09462">LD4LG</a> operate on latent representations of <em>paragraphs</em> of text. But these representations can be pretty fragile ‚Äî even small noise can cause abrupt semantic shifts during reverse diffusion. <strong>My own paper</strong> <a href="https://arxiv.org/abs/2412.11333">SLD</a> tackles this problem <em>cleverly</em> by introducing text-segmentation and improved representation learning techniques. Also worth checking out: Meta‚Äôs <a href="https://arxiv.org/pdf/2412.08821">Large Concept Model</a> ‚Äî arguably the <em>ultimate form</em> of pre-trained DLMs following this path.</p> </li> </ul> <hr/> <p><strong>Discrete Diffusion over Tokens</strong></p> <p>Some <em>bold geniuses</em> thought: ‚ÄúHey, if tokens are discrete, why not make the diffusion process discrete too?‚Äù</p> <p>And so ‚Äî we now have discrete diffusion over categorical supports. <a href="https://arxiv.org/pdf/2102.05379">Eminel et al., 2021</a> extended diffusion to handle categorical data like text. Here, each token is represented as a probability mass vector over the vocabulary \(p \in \mathbb{R}^{V}\), where \(V\) is the vocabulary size. We use transition matrices \(\mathbf{Q}\) to model token-level transitions over timesteps, e.g.,</p> \[q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \text{Categorical}(\mathbf{x}_t; \mathbf{p} = \mathbf{x}_{t-1}\mathbf{Q}_t)\] <p>Models like <a href="https://arxiv.org/abs/2107.03006">D3PM</a> and <a href="https://arxiv.org/pdf/2310.16834">SEDD</a> (<em>ICML 2024 Best Paper</em> üèÜ) follow this path. More commonly, text diffusion models define analogous discrete ‚Äúnoising‚Äù or ‚Äúcorruption‚Äù processes. Instead of adding mathematical noise, the forward process might involve:</p> <ul> <li> <p><strong>Masking</strong>:<br/> Randomly replacing tokens with a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token, with the amount of masking increasing over diffusion steps.<br/> (Example: <a href="https://arxiv.org/pdf/2502.09992">LLaDA</a> ‚Äî an 8B-parameter pre-trained DLM that‚Äôs currently trending.)</p> </li> <li> <p><strong>Token Substitution</strong>:<br/> Randomly replacing tokens with other tokens from the vocabulary.</p> </li> <li> <p><strong>Hybrids</strong>:<br/> Combining masking, substitution, and other discrete corruption methods.</p> </li> </ul> <hr/> <p>üî•üñºÔ∏èüëä <strong>The Maverick ‚Äî Text as Image</strong></p> <p><em>‚ÄúDiscrete text? What text? It‚Äôs an image!‚Äù</em> ü§£</p> <p>Instead of dealing with the discrete gap, <a href="https://arxiv.org/pdf/2304.12519">GlyphDiffusion</a> bypasses it entirely ‚Äî by rendering the target text as glyph <strong>images</strong> containing visual language content. (<em>Yes, seriously. I like the idea very much. I personally wish you to check this out.</em>)</p> <hr/> <p>In all these methods, the reverse process becomes about <strong>undoing</strong> specific types of corruption. For instance, the model learns to predict the original tokens at <code class="language-plaintext highlighter-rouge">[MASK]</code> positions, or correct randomly substituted tokens, gradually refining the sequence from a highly corrupted mess back into coherent text. So while the <em>core idea</em> of diffusion (iterative refinement from noise) stays the same, the <em>mechanisms</em> for forward (corruption) and reverse (denoising) processes have to be <strong>carefully adapted</strong> for the discrete world of language.</p> <hr/> <p>Now, I‚Äôll pick representative works from each paradigm to explain DLMs in more detail. <span style="color:blue">For each paradigm, I‚Äôll introduce key papers, explain the mechanisms, and point you to off-the-shelf pre-trained models you can try out yourself!</span></p> <h4 id="embedding-level-diffusion--where-it-begins"><strong>Embedding-Level Diffusion ‚Äî Where It Begins</strong></h4> <h5 id="1-token-level-embeddings"><strong>1. Token-Level Embeddings</strong></h5> <p><em>As far as I know</em>, <a href="https://arxiv.org/abs/2205.14217">Diffusion-LM</a> is probably the first influential work that kicked off the DLM era üéâ. Suppose we have a sequence of words: \(\mathbf{w} = \{w_1, w_2, \ldots, w_n\}\) An embedding function maps each word into a vector: \(Emb(w_i) \in \mathbb{R}^d\) Thus, the entire sequence is encoded into: \(\mathbf{x}_0 = Emb(\mathbf{w}) \in \mathbb{R}^{n \times d}\)</p> <p>Awesome! Now we have a <strong>continuous</strong> space where we can run good old conventional diffusion models.<br/> (We use the typical simplified KL-divergence term from the evidence lower bound ‚Äî which I won‚Äôt rehash here ‚Äî to derive the loss.)</p> <p>Specifically, the training objective is:</p> \[\begin{equation} \mathcal{L}_{simple}(\mathbf{x}_0) = \sum_{t=1}^T \underset{q(\mathbf{x}_t \mid \mathbf{x}_0)}{\mathbb{E}} \left[ \|\mu(\mathbf{x}_t, t) - \hat{\mu}(\mathbf{x}_t, \mathbf{x}_0)\|^2 \right] \end{equation}\] <hr/> <p>But hold on ‚Äî we can‚Äôt forget about converting embeddings <strong>back</strong> into discrete tokens! You might think: <em>‚ÄúEasy, let‚Äôs just use another function to transform them back.‚Äù</em> And‚Ä¶ you‚Äôd be mostly right. In Li‚Äôs implementation, they model these steps into the diffusion process as an <strong>extra timestep</strong>. As shown in the figure below (üëÄ), the forward process includes an additional Markov transition to obtain embeddings:</p> \[q_{\phi}(\mathbf{x}_0 \mid \mathbf{w}) = \mathcal{N}(Emb(\mathbf{w}); \sigma_0^2 I)\] <p>Then, in the reverse process, you have an <strong>additional trainable rounding step</strong>, parameterized by:</p> \[p_{\theta}(\mathbf{w} \mid \mathbf{x}_0) = \prod_{i=1}^n p_{\theta}(w_i \mid x_i)\] <p>where each \(p_{\theta}(w_i \mid x_i)\) is a simple softmax distribution over the vocabulary.</p> <hr/> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/diffusion-lm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. A graphical model representing the forward and reverse diffusion processes for Diffusion-LM. (Image source: <a href="https://arxiv.org/abs/2205.14217">Li et al. 2022</a>) </div> <p>Then we can adjust the loss function accordingly. For end-to-end training, we arrive at the final loss function shown below. During inference (i.e., the reverse process), you sample a random embedding sequence containing \(n\) token embeddings ‚Äî same as during training ‚Äî and gradually remove the noise, step by step. You‚Äôre always predicting all \(n\) embeddings together, since the diffusion model expects fixed-shape inputs and outputs. (<em>Kind of wasteful if your sequence is shorter than \(n\), right? We‚Äôll talk about that later.</em>)</p> \[\begin{equation} \mathcal{L}_{simple}^{e2e}(\mathbf{w}) = \underset{q_{\phi}(\mathbf{x}_{0:T} | \mathbf{w})}{\mathbb{E}} \left[\underbrace{\mathcal{L}_{simple}(\mathbf{x}_0)}_{\text{diffusion Loss}} + ||Emb(\mathbf{w}) - \overbrace{\mu_{\theta}(\mathbf{x}_1, 1)}^{\text{predicted }\mathbf{x}_0}||^2 - \underbrace{\log p_{\theta}(\mathbf{w} | \mathbf{x}_0)}_{\text{rounding}} \right] \end{equation}\] <p>So, everything seems super straightforward, right? Or‚Ä¶ does it? Unfortunately, no üòÖ. The conversion between continuous embedding space and discrete tokens is actually <strong>non-trivial</strong> ‚Äî and harder than you might think. <span style="color:red"> This rounding step is a key challenge in token embedding-level diffusion models. Discretization can introduce errors that accumulate across the diffusion process, since the embedding space isn‚Äôt uniformly filled with valid tokens. </span> <em>Well, isn‚Äôt this just the notorious data sparsity problem making a comeback?</em></p> <hr/> <p>In the paper, there‚Äôs a whole section dedicated to techniques for reducing <strong>rounding error</strong> and producing better outputs. For instance:</p> <ul> <li>They use the <strong>reparameterization trick</strong> to ensure every term in the loss explicitly models \(\mathbf{x}_0\).</li> <li>They also introduce a <strong>clamping trick</strong>, which maps each predicted vector \(\mathbf{x}_t\) to its nearest word embedding in every reverse sampling step.</li> </ul> <p>Still, a lot of work remains to be done here if we want to really boost generation quality.</p> <hr/> <p>Back to the model itself, with the diffusion pipeline, you can do the fancy conditioning and controlled generation during your inference now. For example, you could have a separate neural network classifier and a class condition \(\mathbf{c}\). During the backward process, you obtain the \(\mathbf{x}_{t-1}\) with respect to the posterior probability using the gradient update below.</p> \[\begin{equation} \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{c}) = \nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{x}_{t-1} | \mathbf{x}_{t}) + \underbrace{\nabla_{\mathbf{x}_{t-1}} \log p(\mathbf{c} | \mathbf{x}_{t-1})}_{\text{classifier guidance}} \end{equation}\] <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/diffusion-lm-classifier-480.webp 480w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-800.webp 800w,/assets/img/diffusionlm_blog/diffusion-lm-classifier-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/diffusion-lm-classifier.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. For controllable generation, we iteratively perform gradient updates on these continuous latents to optimize for fluency (parametrized by Diffusion-LM) and satisfy control requirements (parametrized by a classifier). (Image source: <a href="https://arxiv.org/abs/2205.14217">Li et al. 2022</a>) </div> <p>The paper also provides a bunch of experiments on controlled generation ‚Äî including semantics, length, part-of-speech, and more.<br/> But I want to highlight <strong>infilling</strong> specifically, because it‚Äôs super neat. üß© In this setting, during inference, instead of denoising <em>all</em> embeddings, some context embeddings are <strong>given and fixed</strong>. For example: \(\mathbf{x}_t =\) <code class="language-plaintext highlighter-rouge">[w_1] [MASK] [w_2]</code>. The diffusion model is told to only generate the noisy token in the middle. This is done by masking the gradients of the fixed tokens ‚Äî so they stay untouched ‚Äî while still using them as <strong>context</strong> during the reverse sampling process. In other words, the fixed tokens act as <strong>classifier-free guidance</strong>. And <em>you</em>, my clever reader, have probably already realized: this setup makes it easy to model traditional sequence-to-sequence tasks ‚Äî just give the input as the left context!</p> <hr/> <p><a href="https://arxiv.org/pdf/2212.11685">GENIE</a> is (<em>again, as far as I know</em>) the <strong>first pre-trained DLM</strong> to follow this token embedding-level diffusion path. It uses a <strong>continuous paragraph denoising</strong> objective for pretraining.</p> <p>The idea:</p> <ul> <li>Apply a hybrid noising process to the original text ‚Äî including token masking (like <code class="language-plaintext highlighter-rouge">[w_1] [MASK] [w_2]</code>) and forward diffusion sampling.</li> <li>Then train the model to recover the clean original text from the noisy version.</li> </ul> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GENIE-480.webp 480w,/assets/img/diffusionlm_blog/GENIE-800.webp 800w,/assets/img/diffusionlm_blog/GENIE-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/GENIE.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. The framework of GENIE. Source sequence is encoded as the condition of the transformer DLM through cross attention. DLM restores the randomly initial Gaussian noise to the output text through the iterative denoising and grounding process (Image source: <a href="https://arxiv.org/pdf/2212.11685">Lin et al., 2022</a>) </div> <p>Notably, GENIE doesn‚Äôt use infilling as its default sequence-to-sequence generation method. Instead, it follows more of an <strong>Encoder-Decoder</strong> approach (yep, think BART or T5) ‚Äî which is actually another form of <strong>classifier-free guidance</strong>. The input is fed to the diffusion model, a transformer in this case, as cross-attention targets. Similar rounding techniques are applied here too: GENIE uses an efficient <strong>KNN (k-nearest neighbors) algorithm</strong> to retrieve the closest word embedding for each token during reverse sampling, then rounds to the nearest learned word embedding. This rounding step helps map noisy continuous vectors back to valid discrete tokens more effectively ‚Äî though, as always, there‚Äôs still room for improvement!</p> <h5 id="2-higher-level-embeddings"><strong>2. Higher-Level Embeddings</strong></h5> <p>To address the <strong>rounding error</strong> problem when translating predicted word embeddings back to discrete tokens, researchers have explored bringing diffusion into a <strong>higher-level semantic latent space</strong> ‚Äî like at the sentence or paragraph level. In this setup, the diffusion model doesn‚Äôt operate directly on word embeddings. Instead, it predicts a latent representation of an entire piece of text. Then, a separate <strong>autoregressive decoder</strong> is used to decode that latent into natural language. One representative example is the work by Lovelace et al., <strong>Latent Diffusion for Language Generation</strong> (<a href="https://arxiv.org/abs/2212.09462">LD4LG</a>). We‚Äôll use this to walk through how the concept works.</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LD4LG-480.webp 480w,/assets/img/diffusionlm_blog/LD4LG-800.webp 800w,/assets/img/diffusionlm_blog/LD4LG-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/LD4LG.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Overview of Latent Diffusion For Language Generation (Image source: <a href="https://arxiv.org/abs/2212.09462">Lovelace et al., 2023</a>) </div> <p>First, an encoder model (e.g., BART or T5 Encoder), denoted as \(Enc()\), is used to convert a piece of text \(\mathbf{w} = \{w_1, \ldots, w_n\}\) into hidden states: \(Enc(\mathbf{w}) \in \mathbb{R}^{n \times h_{lm}}\) where \(h_{lm}\) is the hidden size of the encoder (typically 768 or 1024, etc.).</p> <p>Notice that \(n\) here represents the <strong>variable token length</strong> of the text, but diffusion models usually require a <strong>fixed-shape</strong> representation. In token-level embedding diffusion, this imposes a hard constraint on the context window size. For higher-level embeddings, we use an additional <strong>pooling/compression unit</strong> \(f()\) to project the hidden states into a fixed-length latent: \(\mathbf{z} = f(Enc(\mathbf{w})) \in \mathbb{R}^{k \times h_{rep}}\) where \(k\) and \(h_{rep}\) are hyperparameters defining the shape of the latent space. Typically, you want \(k &lt; n\) and \(h_{rep} \ll h_{lm}\) ‚Äî because you don‚Äôt want a sparse, oversized latent. A more compact latent space helps the diffusion model learn the distribution more effectively.</p> <hr/> <p>Then, the diffusion model \(R(\cdot \mid \theta)\) (usually a <strong>diffusion transformer</strong>, like <a href="https://arxiv.org/abs/2212.09748">DiT</a>), is deployed to learn how to recover \(\mathbf{z}_0\) from its noised version \(\mathbf{z}_t\) over the forward diffusion process ‚Äî as usual.</p> \[\begin{equation} \mathcal{L}(\theta_{R}) = \sum_{t=1}^T \underset{q(\mathbf{z}_{t} | \mathbf{z}_0)} {\mathbb{E}} \left\| \hat{\mathbf{z}}_{t} - \mathbf{z}_t \right\|^2_2 \end{equation}\] <p>For inference, you start by sampling a latent representation from Gaussian noise, then run the reverse process as usual \(\hat{\mathbf{z}}_{t-1} = R(\hat{\mathbf{z}}_t, t; \theta_R)\) Conditioning (e.g., class labels, prompts) can also be added to both the forward and reverse processes ‚Äî just like in other diffusion models. Next, we apply a <strong>reconstruction unit</strong> \(g()\) to project the denoised latent state back into the hidden dimension of the decoder LM. Then, the decoder generates text as follows: \(Dec(g(\mathbf{z}_0))\) Since we‚Äôre doing diffusion over the <strong>continuous latent space of high-level semantics</strong>, we effectively bypass the whole <strong>rounding</strong> phase from token-level embedding models.</p> <hr/> <p><strong>BUT</strong>, is it that simple? Actually‚Ä¶ no ‚òπÔ∏è. <span style="color:red"> It‚Äôs non-trivial to find a good latent representation for text ‚Äî let alone generate one from noisy examples. </span> For example, take the sentence: <code class="language-plaintext highlighter-rouge">"It is sunny today"</code> Now, if I perturb its latent representation just a bit, what should that mean? Well, this is <em>semantic</em> diffusion, right? So a small change should preserve the <strong>meaning</strong>, not just the surface form. We‚Äôd hope to get something like: <code class="language-plaintext highlighter-rouge">"Today is sunny"</code> or <code class="language-plaintext highlighter-rouge">"Today has sun"</code> Not: <code class="language-plaintext highlighter-rouge">"It isn't sunny today"</code> or <code class="language-plaintext highlighter-rouge">"It is savvy tody"</code> üôÉ The latter examples may look similar in terms of characters or token embeddings, but they‚Äôre clearly <strong>not</strong> semantically close.</p> <p>And that‚Äôs the crux of the problem: the <strong>meaning</strong> of even a short paragraph is super rich and subtle. How are we supposed to regularize a latent space that captures <em>that</em>? This poses a major challenge for high-level semantic diffusion ‚Äî especially when it comes to <strong>long-form generation</strong>, as it usually packs more and complex meanings.</p> <p>So‚Ä¶ back to square one: <strong>What‚Äôs the definition of a good latent representation for high-level semantic diffusion?</strong> <a href="https://arxiv.org/pdf/2306.02531">Zhang et al., 2024</a> gives a formal definition of this, describing three key desiderata:</p> <hr/> <p><strong>1. Low Conversion Error</strong><br/> Given a piece of text \(\mathbf{w}\), we encode it into latent representations and decode it back: \(\tilde{\mathbf{w}} = Dec(g(f(Enc(\mathbf{w}))))\) The difference between the original \(\mathbf{w}\) and the reconstructed \(\tilde{\mathbf{w}}\) should be minimal ‚Äî ideally none. However, this is <strong>hard</strong> when \(\mathbf{w}\) is long. Remember, \(k\) and \(h_{rep}\) are fixed hyperparameters. So during projection, information gets compressed ‚Äî and longer sequences suffer more loss. (<em>Makes sense, right? The longer the sequence, the more stuff you have to cram into a fixed-size box!</em> üì¶)</p> <hr/> <p><strong>2. Local Smoothness</strong><br/> Imagine someone stuttering or dropping a few minor words while speaking ‚Äî you can usually still understand them. Similarly, given a piece of text \(\mathbf{w}\) and a slightly varied version \(\mathbf{w'}\), their encoded latent representations should be <strong>close</strong> to each other: \(\mathbf{z}_{w} \approx \mathbf{z}_{w'}\) This ensures the latent space is locally smooth, tolerant to small, surface-level changes without drastic semantic shifts.</p> <hr/> <p><strong>3. Distributional Smoothness</strong><br/> In the high-level latent space, we want meanings of paragraphs to be <strong>smoothly distributed</strong>. That is:</p> <ul> <li>A piece of text \(\mathbf{w}\) and its paraphrases should have nearby latent vectors.</li> <li>Small perturbations in latent space should preserve meaning.</li> <li>Texts with very different meanings should be <strong>far apart</strong> in latent space.</li> </ul> <p>Sounds good ‚Äî but <strong>super hard</strong> in practice, especially for long-form text! Longer sequences carry multiple complex ideas, making the latent space messy and harder to regularize. If you increase the size of the latent space to capture all that complexity, the diffusion model faces another challenge: learning a distribution \(p(\mathbf{z})\) that is highly <strong>multimodal</strong> or has a large <strong>Lipschitz constant</strong> ‚Äî meaning the density function can change very abruptly, which is nasty for diffusion models to handle.</p> <hr/> <p>So the truth is: without proper regularization, the learned latent distribution can become <strong>fragile</strong> ‚Äî small perturbations might cause abrupt semantic shifts, making life hard for the diffusion model and <strong>catastrophically corrupting</strong> the quality of the decoded text. Yikes.</p> <p>So‚Ä¶ how do we fix this?</p> <hr/> <p><a href="https://arxiv.org/pdf/2412.11333">Zhu et al., 2024</a> (<em>yeah that‚Äôs me ü§™</em>) proposes <strong>Segment-Level Diffusion (SLD)</strong>. Inspired by the concept of <strong>patching</strong> in image generation, we ‚Äúpatch‚Äù the text into coherent segments ‚Äî like individual <strong>sentences</strong>, <strong>dialogue turns</strong>, or <strong>utterances</strong>. This gives us much better control over both the <strong>length</strong> of each segment and the <strong>semantic scope</strong> of each latent/</p> <hr/> <p>We further regulate the latent representations by doing additional training for representation learning, using <strong>contrastive learning</strong>, and <strong>adversial noise preturbation</strong>, ensuring local and distributional smoothness.</p> <p>These tricks ensure both <strong>local</strong> and <strong>distributional smoothness</strong>, just like we talked about earlier. The diffusion model then learns to predict <strong>multiple latent representations</strong>, with <strong>one-to-one correspondence</strong> to each segment. Each segment‚Äôs latent can then be <strong>decoded independently ‚Äî and in parallel!</strong> That means better generation quality <em>and (maybe)</em> faster inference (<em>in theory, we will talk about this in the end</em>). üí®</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/SLD-480.webp 480w,/assets/img/diffusionlm_blog/SLD-800.webp 800w,/assets/img/diffusionlm_blog/SLD-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/SLD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Overview of the training pipeline of SLD. In the first stage, gold output is divided into segments. In the second stage, we use contrastive and adversarial learning to ensure latent representations are robust to drastic semantic changes. Finally, we train a diffusion model as an inherent semantic planner conditioned on given inputs. (Image source: <a href="https://arxiv.org/pdf/2412.11333">Zhu et al., 2024</a>) </div> <p>And you can see the importance of representation learning in the visualization below. Before the representation learning, the desired cluster of sentences (ROCStories) are not very distinguishable from other texts (CNN/Daily Mail), which makes the model susceptible to abrupt semantic changes during diffusion process. With regularization, it‚Äôs much better. Adversial noise is to ensure we enhance the generation quality even better.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/SLD_REP-480.webp 480w,/assets/img/diffusionlm_blog/SLD_REP-800.webp 800w,/assets/img/diffusionlm_blog/SLD_REP-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/SLD_REP.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparison of PCA 2D projections of latent representations for sampled segmented sentences from ROCStories (Blue), their paraphrases (Green), and out-of-domain (OOD) sentences sampled from CNN/Daily Mail (Orange) under three training paradigms: Vanilla training, Noise Robust training, and Noise Robust + Contrastive learning. The red trajectory illustrates the denoising path of the sentence 'David noticed he had put on a lot of weight recently.' The trajectory is annotated with noise ratios, where 1.0 (Lighter Red) represents pure Gaussian noise and 0.0 (Darker Red) indicates no noise. (Image source: <a href="https://arxiv.org/pdf/2412.11333">Zhu et al., 2024</a>) </div> <p>A contemporary work from Meta, <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/">Large Concept Model</a>, uses a similar paradigm. They perform diffusion over <strong>concepts</strong> ‚Äî which is pretty much the same idea as <strong>segments</strong> in my work. Definitely check out their paper! They provide a model pre-trained on way more data than I had access to.</p> <hr/> <p><span style="color:gray"> <em>Fun fact:</em><br/> I submitted my paper for the <strong>15th of Dec 2024 ARR</strong>, and they released theirs on the <strong>12th of Dec</strong>. I‚Äôd be lying if I said I wasn‚Äôt a <em>bit</em> frustrated‚Ä¶ But hey ‚Äî this also proves that <strong>this idea, is very promising</strong>! üöÄ Hope this whole discussion has been helpful to you! </span></p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LCM-480.webp 480w,/assets/img/diffusionlm_blog/LCM-800.webp 800w,/assets/img/diffusionlm_blog/LCM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/LCM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Left: visualization of reasoning in an embedding space of concepts (task of summarization). Right: fundamental architecture of an Large Concept Model (LCM). Note that the LCM is a diffusion model! (Image source: <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/"> LCM Team 2024</a>) </div> <h4 id="discrete-diffusion-over-tokens"><strong>Discrete Diffusion over Tokens</strong></h4> <p>In this section, we‚Äôll dive into how <strong>diffusion over discrete tokens</strong> is done ‚Äî focusing on the <strong>Masked Diffusion Model (MDM)</strong> as introduced by <a href="https://arxiv.org/abs/2406.07524v2">Sahoo et al.</a> in <em>Masked Diffusion Language Models</em>.</p> <p>This will be our main example, but I also encourage you to check out:</p> <ul> <li><a href="https://arxiv.org/abs/2107.03006">D3PM</a></li> <li><a href="https://arxiv.org/pdf/2310.16834">SEDD</a> (<em>ICML 2024 Best Paper!</em> üèÜ)</li> </ul> <p>However, fair warning: both are a bit mathematically dense and terse for a light intro blog like this one ‚Äî so we‚Äôll keep things digestible here. ü´†</p> <hr/> <p>As mentioned earlier, <a href="https://arxiv.org/abs/2107.03006">D3PM</a> introduces a <strong>Markov forward process</strong> over tokens, using a sequence of <strong>categorical distributions</strong> constructed through multiplication of transition matrices \(\mathbf{Q}_t\) across \(T\) discrete timesteps. Concretely, we have a series of matrix multiplications:</p> \[\begin{equation} \mathbf{x}_T = \mathbf{Q}_{T-1} \cdot \mathbf{Q}_{T-2} \cdots \mathbf{Q}_1 \cdot \mathbf{x}_0 \end{equation}\] <p>This gradually transforms the initial sequence \(\mathbf{x}_0\) into a <strong>stationary distribution</strong> ‚Äî i.e., full corruption.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/MDLM-480.webp 480w,/assets/img/diffusionlm_blog/MDLM-800.webp 800w,/assets/img/diffusionlm_blog/MDLM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/MDLM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> (Left) Masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling.(Bottom Right) Perplexity (PPL) on One Billion Words benchmark. (Image source: <a href="https://arxiv.org/abs/2406.07524v2"> Sahoo et al., 2024</a>) </div> <p>In their work, tokens are represented as: \(\mathbf{x} \in \mathcal{V}\) where \(\mathcal{V}\) is the set of all one-hot vectors of the vocabulary, and \(|\mathcal{V}| = K\).</p> <p>They define \(\text{Cat}(\cdot; \boldsymbol{\pi})\) as a <strong>categorical distribution</strong> over \(K\) token classes, with class probabilities given by \(\boldsymbol{\pi} \in \Delta^K\) ‚Äî the <strong>K-simplex</strong> (i.e., the space of valid probability vectors over \(K\) classes). They also define a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token: \(\mathbf{m} \in \mathcal{V}\)</p> <hr/> <p>During the <strong>forward process</strong>, they interpolate discrete diffusion by gradually converting \(\mathbf{x}\) into increasingly noisy variables \(\mathbf{z}_t\). The marginal distribution of \(\mathbf{z}_t\) conditioned on the original \(\mathbf{x}\) is:</p> \[\begin{equation} q(\mathbf{z}_t \mid \mathbf{x}) = \text{Cat}(\mathbf{z}_t; \alpha_t \mathbf{x} + (1 - \alpha_t)\boldsymbol{\pi}) \end{equation}\] <p>Here, \(\alpha_t\) is a scalar from the <strong>noise schedule</strong>, just like in standard diffusion models.</p> <hr/> <p>In the <strong>masked diffusion</strong> variant, we set: \(\boldsymbol{\pi} = \mathbf{m}\)</p> <p>This means that at each timestep \(t\), the input token \(\mathbf{x}\) has a chance of being replaced by the special <code class="language-plaintext highlighter-rouge">[MASK]</code> token \(\mathbf{m}\). Once a token is masked ‚Äî i.e., it transitions to \(\mathbf{m}\) at some time \(t\) ‚Äî it <strong>stays masked</strong> for all subsequent timesteps. ü´• (No going back from <code class="language-plaintext highlighter-rouge">[MASK]</code> ‚Äî it‚Äôs a one-way trip.)</p> \[\begin{equation} q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x}) = \begin{cases} \text{Cat}(\mathbf{z}_s; \mathbf{z}_t), &amp; \mathbf{z}_t \neq \mathbf{m},\\ \text{Cat}\left( \mathbf{z}_s; \frac{(1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t)\mathbf{x}}{1 - \alpha_t} \right), &amp; \mathbf{z}_t = \mathbf{m}. \end{cases} \end{equation}\] <p>So, consequently, for the reverse diffusion process, we train a network to do \(p_{\theta} (\mathbf{z}_s | \mathbf{z}_t)\) to convert masked tokens back to concrete tokens.</p> \[\begin{equation} p_\theta(\mathbf{z}_s \mid \mathbf{z}_t) = q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x} = \mathbf{x}_\theta(\mathbf{z}_t, t)) = \begin{cases} \text{Cat}(\mathbf{z}_s; \mathbf{z}_t), &amp; \mathbf{z}_t \neq \mathbf{m}, \\ \text{Cat}\left(\mathbf{z}_s; \frac{(1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t)\mathbf{x}_\theta(\mathbf{z}_t, t)}{1 - \alpha_t} \right), &amp; \mathbf{z}_t = \mathbf{m}. \end{cases} \end{equation}\] <p>Great, right? In this case, they‚Äôve successfully extended diffusion from the <strong>continuous</strong> into the <strong>discrete</strong> domain. üéâ I‚Äôve omitted a few technical details ‚Äî but you can check those out in their excellent <a href="https://s-sahoo.com/mdlm/">blog</a> or the full <a href="https://arxiv.org/abs/2406.07524v2">paper</a>.</p> <hr/> <p>That said, note that the current design has <strong>some limitations</strong>. <em>While the prototype naturally inherits the constraints of an early-stage system, it still marks a major step forward in the field.</em> One issue is during <strong>decoding</strong>: Once a token is unmasked, it stays fixed. This isn‚Äôt ideal ‚Äî especially in the <strong>early stages of reverse diffusion</strong>, when the paragraph is still mostly noise and the few decoded tokens are likely <em>not</em> optimal. But since they‚Äôre locked in place, the model must condition future generations on possibly bad guesses ‚Äî causing the dreaded <strong>error propagation</strong> problem all over again. üòû</p> <hr/> <p><a href="https://arxiv.org/abs/2302.05737">Lin et al.</a>, from <a href="https://ikekonglp.github.io/">Lingpeng Kong‚Äôs group</a> (<em>I really like their work!</em>) propose a fix for this using a <strong>routing</strong> mechanism. The idea? Even during decoding, a previously decoded token can be <strong>remasked</strong> (i.e., turned back into <code class="language-plaintext highlighter-rouge">[MASK]</code>) if the model‚Äôs confidence in that token is low. This allows the model to revisit and revise its decisions ‚Äî bringing in a form of <strong>iterative refinement</strong> that‚Äôs more faithful to the spirit of diffusion.</p> <div class="row mt-2"> <div class="col-sm-12 col-md-10 mt-6 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/LLaDA.svg" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/LLaDA.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> (a) Pre-training. LLaDA is trained on text with random masks applied independently to all tokens at the same ratio t ‚àº U[0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion process from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies. (Image source: <a href="https://arxiv.org/abs/2502.09992"> Nie et al., 2025</a>) </div> <p>Finally, <a href="https://arxiv.org/abs/2502.09992"><strong>LLaDA 8B</strong></a> combines <strong>both</strong> techniques mentioned above ‚Äî Masked Diffusion Language Modeling (<strong>MDLM</strong>) and <strong>routing</strong> ‚Äî demonstrating the real potential of MDM as a new paradigm for <strong>pre-trained language models</strong>. The results? LLaDA achieves <strong>on-par</strong> ‚Äî and in some cases <strong>superior</strong> ‚Äî generation quality compared to autoregressive LLMs of the same size. Amazing, isn‚Äôt it? ü§Ø</p> <p>Try <a href="https://huggingface.co/spaces/multimodalart/LLaDA">it</a> out yourself!</p> <h4 id="text-in-image-diffusion-brain-teaser"><strong>Text-in-Image Diffusion? (Brain-teaser)</strong></h4> <p>Think <strong>computer vision</strong> for a moment! üëÄ These days, NLP and CV have been borrowing ideas from each other all the time ‚Äî just look at <a href="https://arxiv.org/abs/2404.02905">VAR</a>, which brings autoregressive generation into image synthesis. So hey, if we‚Äôre already using <strong>diffusion</strong> (the most trending paradigm in CV), why not push the crossover even further? I‚Äôll skip over some amazing CV works like <a href="https://github.com/deep-floyd/IF">DeepFloyd IF</a> for the sake of conciseness. Instead, here‚Äôs a fun little brain-teaser to wrap things up: <a href="https://arxiv.org/pdf/2304.12519v2"><strong>GlyphDiffusion</strong></a>.</p> <hr/> <p>The key idea is <em>wild</em> but clever:</p> <ul> <li>Render the target text as a <strong>glyph image</strong> ‚Äî that is, an actual visual representation of the characters ‚Äî</li> <li>and treat conditional text generation as a <strong>glyph image generation</strong> task.</li> </ul> <p>Now that you‚Äôre working in the <strong>image domain</strong>, you can naturally apply all your favorite continuous diffusion tricks ‚Äî <strong>no discrete token rounding or masking needed</strong>! <em>Let‚Äôs not worry (yet) about whether this is scalable or practical.</em> At the very least‚Ä¶ it‚Äôs fun. üòÑ</p> <div class="row mt-2"> <div class="col-sm-10 col-md-8 mt-4 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusionlm_blog/GlyphDiffusion-480.webp 480w,/assets/img/diffusionlm_blog/GlyphDiffusion-800.webp 800w,/assets/img/diffusionlm_blog/GlyphDiffusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/diffusionlm_blog/GlyphDiffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GlyphDiffusion generate patches of image containing visual language contents. (Image source: <a href="https://arxiv.org/abs/2304.12519v2"> Li et al., 2023</a>) </div> <h4 id="challenges-and-opportunities"><strong>Challenges and Opportunities</strong></h4> <p>You might be wondering:</p> <blockquote> <p><em>‚ÄúHey, after reading through this blog, DLMs sound pretty well-established ‚Äî we‚Äôve even got pre-trained models already! So what‚Äôs stopping us from using them more widely?‚Äù</em></p> </blockquote> <p>Great question. The short answer? <strong>Speed</strong>.</p> <hr/> <p>The major challenge for DLMs right now is <strong>inference efficiency</strong> and <strong>sampling cost</strong>. Yes, we‚Äôve talked about how non-autoregressive (NAR) generation could <em>potentially</em> be faster than traditional AR methods ‚Äî but‚Ä¶ we‚Äôre not quite there yet. Autoregressive models have tricks like <strong>KV caching</strong>, which significantly boost decoding speed. Unfortunately, these acceleration methods aren‚Äôt applicable (yet) for NAR models like DLMs. Right now, naive DLM implementations are about <strong>50√ó slower</strong> than AR baselines. üê¢</p> <hr/> <p><strong>But!</strong> Don‚Äôt be discouraged ‚Äî we‚Äôre making steady progress. There‚Äôs growing work adapting <strong>acceleration techniques</strong> to the diffusion domain, including:</p> <ul> <li><a href="https://openai.com/index/simplifying-stabilizing-and-scaling-continuous-time-consistency-models/"><strong>Consistency Models</strong> (OpenAI)</a></li> <li><a href="https://arxiv.org/abs/2305.04465"><strong>Adaptive Decay Sampling</strong> (Tang et al., 2023)</a></li> <li><a href="https://arxiv.org/abs/2402.07754"><strong>Diffusion of Thoughts</strong> (Ye et al., 2024)</a>, where they use <strong>ODE solvers</strong> to speed up decoding</li> </ul> <p>And hey, some people have <em>already</em> pulled it off! Check out Inception Labs‚Äô <a href="https://chat.inceptionlabs.ai/"><strong>Mercury Coder</strong></a> ‚Äî the first <strong>fast commercial DLM</strong> out in the wild!</p> <hr/> <p>But speed isn‚Äôt everything. There are <strong>so many opportunities</strong> with DLMs that we‚Äôve only just begun to explore:</p> <ul> <li> <p>Since DLMs generate entire sequences holistically, could this change <strong>how we do reasoning</strong>?<br/> Maybe it helps avoid bad intermediate steps and gives better <strong>chain-of-thought</strong> answers.<br/> See: <a href="https://arxiv.org/abs/2402.07754">Diffusion-of-Thought</a>, <a href="https://hkunlp.github.io/blog/2025/dream/">Dream 7B</a></p> </li> <li> <p>DLMs are great at <strong>fine-grained control</strong>, like <strong>in-filling</strong>, making them ideal for generating structured outputs with constraints:<br/> tables, code, logical forms, and more.<br/> Check out: <a href="https://arxiv.org/pdf/2410.20626">TabDiff</a>, <a href="https://arxiv.org/abs/2407.02549">Mario et al., 2024</a></p> </li> <li> <p>The stochasticity of diffusion makes it a natural fit for <strong>data augmentation</strong> ‚Äî especially useful for low-resource settings.<br/> <a href="https://aclanthology.org/2024.emnlp-main.109.pdf">Chen et al., 2024</a> used it to improve <strong>low-resource sentiment classification</strong>.</p> </li> <li> <p>We can also think about how would this change the uncertainty estimation? As you can do sampling more naturally, maybe shed lights on self-consistency and other confidence related decoding (<strong>remasking</strong>).</p> </li> <li> <p>And of course‚Ä¶ <strong>multi-modal dreams</strong>!<br/> Can we use one model to generate <strong>images and text</strong> together?<br/> <a href="https://ai.meta.com/research/publications/transfusion-predict-the-next-token-and-diffuse-images-with-one-multi-modal-model/">Meta‚Äôs Transfusion</a> is already showing promising results.</p> <p><em>Though‚Ä¶ technically they use AR for both images annd text.</em><br/> <em>So when we are doing this the other way around, maybe we should rename it: <strong>FusionTrans‚Ñ¢</strong> or <strong>DiffFormer‚Ñ¢</strong></em> üòé</p> </li> </ul> <hr/> <p>Oh ‚Äî and if you‚Äôre curious to keep up with this fast-growing area, shout out to the people who keep a living list of all known <a href="https://github.com/bansky-cl/diffusion-nlp-paper-arxiv">DLM papers on GitHub</a>. Highly recommended if you want to go down the rabbit hole. üêáüìö</p> <h5 id="epilogue"><strong>Epilogue</strong></h5> <p>I‚Äôll end this introductory blog <strong>without a conclusion</strong> ‚Äî because the <strong>Era of DLM</strong> has only just begun. üöÄ</p> <p>I hope this blog helped you understand DLMs a little better, and maybe even sparked some ideas if you‚Äôre thinking about doing DLM-related research yourself! (<em>Oh gosh, I hope I didn‚Äôt make too many dad jokes or use too many emojis.</em> üòÖ) If you liked this post, I might write more blogs in the future to dive deeper into specific aspects.</p> <hr/> <p>If you found this blog helpfuln and want to do me a favour, you can cite my <a href="https://arxiv.org/abs/2412.11333">Segment-Level Diffusion</a> paper ‚Äî I‚Äôve incorporated most of this blog‚Äôs content into the related work section there. Also, feel free to leave a comment if you have any suggestions for improving the blog. I‚Äôd love to hear from you! üôè</p> <p>Until next time‚Ä¶</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhu2024segmentleveldiffusionframeworkcontrollable</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models}</span><span class="p">,</span> 
  <span class="na">author</span><span class="p">=</span><span class="s">{Xiaochen Zhu and Georgi Karadzhov and Chenxi Whitehouse and Andreas Vlachos}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span><span class="p">=</span><span class="s">{2412.11333}</span><span class="p">,</span>
  <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.11333}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[A gentle, in-depth introduction of existing diffusion language models.]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://spacehunterinf.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://spacehunterinf.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://spacehunterinf.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/tabs</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="905f2b96-acfe-4667-a3aa-2786a4551ec3" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="905f2b96-acfe-4667-a3aa-2786a4551ec3" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="ae23eee8-cb03-49b1-8a32-3ac7ff91117f" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="ae23eee8-cb03-49b1-8a32-3ac7ff91117f" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="5193dca8-5baa-443f-ae3a-222d01fe1359" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="5193dca8-5baa-443f-ae3a-222d01fe1359" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://spacehunterinf.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/typograms</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://spacehunterinf.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://spacehunterinf.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://spacehunterinf.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://spacehunterinf.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">a post with vega lite</title><link href="https://spacehunterinf.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="a post with vega lite"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://spacehunterinf.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://spacehunterinf.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry></feed>